{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import streamlit as st\n",
    "\n",
    "@st.cache_resource\n",
    "def install_packages(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "packages = [\n",
    "    \"certifi\",\n",
    "    \"charset-normalizer\",\n",
    "    \"faiss-cpu\",\n",
    "    \"idna\",\n",
    "    \"numpy\",\n",
    "    \"packaging\",\n",
    "    \"python-dotenv\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"pyarrow\",\n",
    "    \"PyPDF2\",\n",
    "    \"python-docx\",\n",
    "    \"spacy\",\n",
    "]\n",
    "\n",
    "if \"setup_complete\" not in st.session_state:\n",
    "    progress_placeholder = st.empty()\n",
    "    progress_bar = progress_placeholder.progress(0, text=\"Setting up the application...\")\n",
    "    total = len(packages)\n",
    "    for i, package in enumerate(packages, start=1):\n",
    "        progress_bar.progress(int((i - 1) / total * 100), text=f\"Setting up step {i} of {total}...\")\n",
    "        try:\n",
    "            install_packages(package)\n",
    "        except subprocess.CalledProcessError:\n",
    "            progress_placeholder.empty()\n",
    "            st.error(\"Something went wrong while setting things up. Please try again or contact support.\")\n",
    "            break\n",
    "        progress_bar.progress(int(i / total * 100), text=f\"Finished step {i} of {total}\")\n",
    "    else:\n",
    "        progress_placeholder.empty()\n",
    "        st.session_state.setup_complete = True\n",
    "        st.toast(\"You're all set! Choose 'Upload document' to load an RFP or 'Ask a question' to chat. Use the sidebar to switch interface modes and provide any required API keys.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e011114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import html\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Callable\n",
    "\n",
    "from cli_app import (\n",
    "    load_input_text,\n",
    "    extract_questions,\n",
    "    build_docx,\n",
    ")\n",
    "from qa_core import answer_question, collect_relevant_snippets\n",
    "from answer_composer import CompletionsClient, get_openai_completion\n",
    "from input_file_reader.interpreter_sheet import collect_non_empty_cells\n",
    "from rfp_xlsx_slot_finder import ask_sheet_schema\n",
    "from rfp_xlsx_apply_answers import write_excel_answers\n",
    "from rfp_docx_slot_finder import extract_slots_from_docx\n",
    "from rfp_docx_apply_answers import apply_answers_to_docx\n",
    "import my_module\n",
    "from my_module import _classify_intent, _detect_followup, gen_answer\n",
    "MODEL_DESCRIPTIONS = {\n",
    "    \"gpt-4.1-nano-2025-04-14_research\": \"Lighter, faster model\",\n",
    "    \"o3-2025-04-16_research\": \"Slower, reasoning model\",\n",
    "}\n",
    "MODEL_SHORT_NAMES = {\n",
    "    \"gpt-4.1-nano-2025-04-14_research\": \"4.1\",\n",
    "    \"o3-2025-04-16_research\": \"o3\",\n",
    "}\n",
    "MODEL_OPTIONS = list(MODEL_DESCRIPTIONS.keys())\n",
    "FOLLOWUP_DEFAULT_MODEL = \"gpt-4.1-nano-2025-04-14_research\"\n",
    "DEFAULT_MODEL = \"o3-2025-04-16_research\"\n",
    "try:\n",
    "    DEFAULT_INDEX = MODEL_OPTIONS.index(DEFAULT_MODEL)\n",
    "except ValueError:\n",
    "    DEFAULT_INDEX = 0\n",
    "    DEFAULT_MODEL = MODEL_OPTIONS[0]\n",
    "\n",
    "\n",
    "def load_fund_tags() -> List[str]:\n",
    "    path = Path('~/derivs-tool/rfp-ai-tool/structured_extraction/embedding_data.json').expanduser()\n",
    "    try:\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception:\n",
    "        return []\n",
    "    tags = {t for item in data for t in item.get('metadata', {}).get('tags', [])}\n",
    "    return sorted(tags)\n",
    "\n",
    "\n",
    "class OpenAIClient:\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "\n",
    "    def get_completion(self, prompt: str, json_output: bool = False):\n",
    "        return get_openai_completion(prompt, self.model, json_output=json_output)\n",
    "\n",
    "\n",
    "def save_uploaded_file(uploaded_file) -> str:\n",
    "    suffix = Path(uploaded_file.name).suffix\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n",
    "    tmp.write(uploaded_file.read())\n",
    "    tmp.flush()\n",
    "    return tmp.name\n",
    "\n",
    "\n",
    "def build_generator(\n",
    "    search_mode: str,\n",
    "    fund: Optional[str],\n",
    "    k: int,\n",
    "    length: Optional[str],\n",
    "    approx_words: Optional[int],\n",
    "    min_confidence: float,\n",
    "    include_citations: bool,\n",
    "    llm,\n",
    "    extra_docs: Optional[List[str]] = None,\n",
    "):\n",
    "    def gen(question: str, progress: Optional[Callable[[str], None]] = None):\n",
    "        ans, cmts = answer_question(\n",
    "            question,\n",
    "            search_mode,\n",
    "            fund,\n",
    "            k,\n",
    "            length,\n",
    "            approx_words,\n",
    "            min_confidence,\n",
    "            llm,\n",
    "            extra_docs=extra_docs,\n",
    "            progress=progress,\n",
    "        )\n",
    "        if not include_citations:\n",
    "            ans = re.sub(r\"\\[\\d+\\]\", \"\", ans)\n",
    "            return ans\n",
    "        citations = {\n",
    "            lbl: {\"text\": snippet, \"source_file\": src}\n",
    "            for lbl, src, snippet, score, date in cmts\n",
    "        }\n",
    "        return {\"text\": ans, \"citations\": citations}\n",
    "\n",
    "    return gen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_top_preapproved_answers(question: str, hits: List[dict], limit: int = 5) -> List[dict]:\n",
    "    \"\"\"Use the Aladdin completions client to pick the most relevant pre-approved answers.\"\"\"\n",
    "    if len(hits) <= limit:\n",
    "        return hits\n",
    "\n",
    "    formatted = []\n",
    "    for idx, hit in enumerate(hits, 1):\n",
    "        snippet = (hit.get(\"snippet\") or \"\").strip().replace(\"\", \" \")\n",
    "        if len(snippet) > 500:\n",
    "            snippet = snippet[:497] + \"...\"\n",
    "        source = hit.get(\"source\") or \"unknown\"\n",
    "        score = hit.get(\"score\")\n",
    "        if isinstance(score, (int, float)):\n",
    "            score_repr = f\"{score:.3f}\"\n",
    "        else:\n",
    "            score_repr = str(score) if score is not None else \"unknown\"\n",
    "        date = hit.get(\"date\") or \"unknown\"\n",
    "        formatted.append(\n",
    "            f\"{idx}. Source: {source}\\nScore: {score_repr}\\nDate: {date}\\nSnippet: {snippet}\"\n",
    "        )\n",
    "\n",
    "    prompt = (\n",
    "        \"You are ranking pre-approved RFP answers for how well they address a user's question. \"\n",
    "        \"Return a JSON object with a 'top_indices' array listing the five most relevant answer indices (1-based) in order of relevance.\"\n",
    "        f\"\\n\\nQuestion: {question}\"\n",
    "        \"\\n\\nCandidates:\\n\" + \"\\n\\n\".join(formatted)\n",
    "    )\n",
    "\n",
    "    model_name = os.environ.get(\"ALADDIN_RERANK_MODEL\", \"o3-2025-04-16_research\")\n",
    "    try:\n",
    "        client = CompletionsClient(model=model_name)\n",
    "        content, _ = client.get_completion(prompt, json_output=True)\n",
    "        data = json.loads(content or \"{}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"select_top_preapproved_answers failed with {model_name}: {exc}\")\n",
    "        return hits[:limit]\n",
    "\n",
    "    indices = data.get(\"top_indices\") or data.get(\"top\") or data.get(\"indices\") or []\n",
    "    selected = []\n",
    "    seen = set()\n",
    "    for idx in indices:\n",
    "        try:\n",
    "            pos = int(idx)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if 1 <= pos <= len(hits) and pos not in seen:\n",
    "            seen.add(pos)\n",
    "            selected.append(hits[pos - 1])\n",
    "        if len(selected) == limit:\n",
    "            break\n",
    "\n",
    "    if len(selected) < limit:\n",
    "        for hit in hits:\n",
    "            if hit not in selected:\n",
    "                selected.append(hit)\n",
    "            if len(selected) == limit:\n",
    "                break\n",
    "\n",
    "    if not selected:\n",
    "        return hits[:limit]\n",
    "\n",
    "    return selected\n",
    "\n",
    "def main():\n",
    "    st.title(\"RFP Responder\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        div.block-container{\n",
    "            max-width: 750px;\n",
    "            padding-top: 2rem;\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage\"]{\n",
    "            border-radius: 0.5rem;\n",
    "            padding: 1rem;\n",
    "            margin-bottom: 1rem;\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage-user\"]{\n",
    "            background-color: #DCF8C6;\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage-assistant\"]{\n",
    "            background-color: #FFFFFF;\n",
    "        }\n",
    "        div[data-testid=\"stChatInput\"] textarea{\n",
    "            border-radius: 0.5rem;\n",
    "            padding: 0.75rem;\n",
    "        }\n",
    "        @keyframes shimmer{\n",
    "            0%{background-position:-1000px 0;}\n",
    "            100%{background-position:1000px 0;}\n",
    "        }\n",
    "        .shimmer{\n",
    "            background:linear-gradient(90deg,#d0d0d0 0%,#b0b0b0 50%,#d0d0d0 100%);\n",
    "            background-size:1000px 100%;\n",
    "            animation:shimmer 2s infinite linear;\n",
    "            -webkit-background-clip:text;\n",
    "            -webkit-text-fill-color:transparent;\n",
    "        }\n",
    "        .hit-card{\n",
    "            border:1px solid #e5e7eb;\n",
    "            border-radius:0.75rem;\n",
    "            padding:0.75rem 1rem;\n",
    "            background-color:#f9fafb;\n",
    "            margin-top:0.75rem;\n",
    "        }\n",
    "        .hit-meta{\n",
    "            font-size:0.85rem;\n",
    "            color:#6b7280;\n",
    "            margin-bottom:0.5rem;\n",
    "        }\n",
    "        .hit-snippet{\n",
    "            font-size:0.95rem;\n",
    "            line-height:1.5;\n",
    "            color:#111827;\n",
    "        }\n",
    "        .hit-score{\n",
    "            font-size:0.8rem;\n",
    "            color:#6b7280;\n",
    "            margin-top:0.25rem;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "    view_mode = st.sidebar.radio(\"Interface mode\", [\"User\", \"Developer\"], index=0)\n",
    "    input_mode = st.radio(\"How would you like to proceed?\", [\"Upload document\", \"Ask a question\"], index=1, horizontal=True)\n",
    "    llm_model = MODEL_OPTIONS[DEFAULT_INDEX]\n",
    "\n",
    "    framework_env = os.getenv(\"ANSWER_FRAMEWORK\")\n",
    "    if framework_env:\n",
    "        if view_mode == \"Developer\":\n",
    "            st.info(f\"Using framework from ANSWER_FRAMEWORK: {framework_env}\")\n",
    "        framework = framework_env\n",
    "    else:\n",
    "        framework = st.selectbox(\"Framework\", [\"aladdin\", \"openai\"], index=0, help=\"Choose backend for language model.\")\n",
    "\n",
    "    if framework == \"aladdin\":\n",
    "        for key, label in [\n",
    "            (\"aladdin_studio_api_key\", \"Aladdin Studio API key\"),\n",
    "            (\"defaultWebServer\", \"Default Web Server\"),\n",
    "            (\"aladdin_user\", \"Aladdin user\"),\n",
    "            (\"aladdin_passwd\", \"Aladdin password\"),\n",
    "        ]:\n",
    "            if os.getenv(key):\n",
    "                if view_mode == \"Developer\":\n",
    "                    st.info(f\"{key} loaded from environment\")\n",
    "            else:\n",
    "                val = st.text_input(label, type=\"password\" if \"passwd\" in key or \"api_key\" in key else \"default\")\n",
    "                if val:\n",
    "                    os.environ[key] = val\n",
    "    else:\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            if view_mode == \"Developer\":\n",
    "                st.info(\"OPENAI_API_KEY loaded from environment\")\n",
    "        else:\n",
    "            api_key = st.text_input(\"OpenAI API key\", type=\"password\", help=\"API key for OpenAI.\")\n",
    "            if api_key:\n",
    "                os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "    if input_mode == \"Upload document\":\n",
    "        uploaded = st.file_uploader(\n",
    "            \"Upload document\",\n",
    "            type=[\"pdf\", \"docx\", \"xls\", \"xlsx\"],\n",
    "            help=\"Upload the RFP or question file (PDF, Word, or Excel).\",\n",
    "        )\n",
    "        if uploaded and Path(uploaded.name).suffix.lower() not in [\".pdf\", \".docx\", \".xls\", \".xlsx\"]:\n",
    "            st.warning(\"Only PDF, Word, or Excel documents are supported.\")\n",
    "            uploaded = None\n",
    "    else:\n",
    "        uploaded = None\n",
    "\n",
    "    if view_mode == \"Developer\":\n",
    "        st.info(\"Search mode fixed to 'both'\")\n",
    "        search_mode = \"both\"\n",
    "        fund = st.selectbox(\n",
    "            \"Fund\", [\"\"] + load_fund_tags(), index=0,\n",
    "            help=\"Filter answers for a specific fund or strategy.\",\n",
    "        )\n",
    "        llm_model = st.selectbox(\n",
    "            \"LLM model\",\n",
    "            MODEL_OPTIONS,\n",
    "            index=DEFAULT_INDEX,\n",
    "            format_func=lambda m: f\"{m} - {MODEL_DESCRIPTIONS[m]}\",\n",
    "            help=\"Model name for generating answers.\",\n",
    "        )\n",
    "        k_max_hits = st.number_input(\"Hits per question\", value=20, help=\"Maximum documents retrieved per question.\")\n",
    "        min_confidence = st.number_input(\"Min confidence\", value=0.0, help=\"Minimum score for retrieved documents.\")\n",
    "        docx_as_text = st.checkbox(\"Treat DOCX as text\", value=False)\n",
    "        docx_write_mode = st.selectbox(\"DOCX write mode\", [\"fill\", \"replace\", \"append\"], index=0)\n",
    "        extra_uploads = st.file_uploader(\n",
    "            \"Additional documents\",\n",
    "            type=[\"pdf\", \"docx\", \"xls\", \"xlsx\"],\n",
    "            accept_multiple_files=True,\n",
    "            help=\"Additional PDF or Word documents to include in search.\",\n",
    "        )\n",
    "        if extra_uploads:\n",
    "            valid_files = []\n",
    "            invalid_files = []\n",
    "            for f in extra_uploads:\n",
    "                if Path(f.name).suffix.lower() in [\".pdf\", \".docx\", \".xls\", \".xlsx\"]:\n",
    "                    valid_files.append(f)\n",
    "                else:\n",
    "                    invalid_files.append(f.name)\n",
    "            if invalid_files:\n",
    "                st.warning(\"Unsupported file types were ignored: \" + \", \".join(invalid_files))\n",
    "            extra_uploads = valid_files\n",
    "    else:\n",
    "        st.markdown(\"### Settings\")\n",
    "        search_mode = \"both\"\n",
    "        fund = st.selectbox(\n",
    "            \"Fund\", [\"\"] + load_fund_tags(), index=0,\n",
    "            help=\"Select fund or strategy context for better answers.\",\n",
    "        )\n",
    "        k_max_hits = 20\n",
    "        min_confidence = 0.0\n",
    "        docx_as_text = False\n",
    "        docx_write_mode = \"fill\"\n",
    "        extra_uploads = st.file_uploader(\n",
    "            \"Additional documents\",\n",
    "            type=[\"pdf\", \"docx\", \"xls\", \"xlsx\"],\n",
    "            accept_multiple_files=True,\n",
    "            help=\"Additional PDF or Word documents to include in search.\",\n",
    "        )\n",
    "        if extra_uploads:\n",
    "            valid_files = []\n",
    "            invalid_files = []\n",
    "            for f in extra_uploads:\n",
    "                if Path(f.name).suffix.lower() in [\".pdf\", \".docx\", \".xls\", \".xlsx\"]:\n",
    "                    valid_files.append(f)\n",
    "                else:\n",
    "                    invalid_files.append(f.name)\n",
    "            if invalid_files:\n",
    "                st.warning(\"Unsupported file types were ignored: \" + \", \".join(invalid_files))\n",
    "            extra_uploads = valid_files\n",
    "\n",
    "    with st.expander(\"More options\"):\n",
    "        if view_mode == \"User\":\n",
    "            llm_model = st.selectbox(\n",
    "                \"Model\",\n",
    "                MODEL_OPTIONS,\n",
    "                index=MODEL_OPTIONS.index(llm_model),\n",
    "                format_func=lambda m: f\"{MODEL_SHORT_NAMES[m]} - {MODEL_DESCRIPTIONS[m]}\",\n",
    "                help=\"Choose which model generates answers.\",\n",
    "            )\n",
    "        length_opt = st.selectbox(\"Answer length\", [\"auto\", \"short\", \"medium\", \"long\"], index=3)\n",
    "        approx_words = st.text_input(\"Approx words\", value=\"\", help=\"Approximate words per answer (optional).\")\n",
    "        include_env = os.getenv(\"RFP_INCLUDE_COMMENTS\")\n",
    "        if include_env is not None:\n",
    "            include_citations = include_env != \"0\"\n",
    "            st.info(f\"Using include citations from RFP_INCLUDE_COMMENTS: {include_citations}\")\n",
    "        else:\n",
    "            include_citations = st.checkbox(\"Include citations\", value=True)\n",
    "        show_live = st.checkbox(\"Show questions and answers during processing\", value=True)\n",
    "\n",
    "\n",
    "    if input_mode == \"Ask a question\":\n",
    "        extra_docs = [save_uploaded_file(f) for f in extra_uploads] if extra_uploads else None\n",
    "        llm = CompletionsClient(model=llm_model) if framework == \"aladdin\" else OpenAIClient(model=llm_model)\n",
    "        gen = build_generator(\n",
    "            search_mode,\n",
    "            fund,\n",
    "            int(k_max_hits),\n",
    "            length_opt,\n",
    "            int(approx_words) if approx_words else None,\n",
    "            float(min_confidence),\n",
    "            include_citations,\n",
    "            llm,\n",
    "            extra_docs,\n",
    "        )\n",
    "        my_module._llm_client = llm\n",
    "        response_mode = st.radio(\n",
    "            \"Response style\",\n",
    "            [\"Generate answer\", \"Closest pre-approved answers\"],\n",
    "            index=0,\n",
    "            horizontal=True,\n",
    "            help=\"Switch between generating an answer or browsing the closest approved responses.\",\n",
    "        )\n",
    "        if \"chat_messages\" not in st.session_state:\n",
    "            st.session_state.chat_messages = []\n",
    "        if \"question_history\" not in st.session_state:\n",
    "            st.session_state.question_history = []\n",
    "        sidebar = st.sidebar.container()\n",
    "        sidebar.markdown(\"### References\")\n",
    "        answer_idx = 0\n",
    "        for msg in st.session_state.chat_messages:\n",
    "            with st.chat_message(msg[\"role\"]):\n",
    "                if \"hits\" in msg:\n",
    "                    st.markdown(msg[\"content\"])\n",
    "                    hits = msg.get(\"hits\") or []\n",
    "                    if hits:\n",
    "                        for i, hit in enumerate(hits, 1):\n",
    "                            snippet = html.escape(hit.get(\"snippet\", \"\"))\n",
    "                            source_name = html.escape(hit.get(\"source\", \"Unknown\"))\n",
    "                            meta_parts = [f\"<strong>{i}. {source_name}</strong>\"]\n",
    "                            score_val = hit.get(\"score\")\n",
    "                            if isinstance(score_val, (int, float)):\n",
    "                                meta_parts.append(f\"Score {score_val:.3f}\")\n",
    "                            elif score_val:\n",
    "                                meta_parts.append(f\"Score {html.escape(str(score_val))}\")\n",
    "                            date_val = hit.get(\"date\")\n",
    "                            if date_val:\n",
    "                                meta_parts.append(html.escape(str(date_val)))\n",
    "                            meta_line = \" · \".join(meta_parts)\n",
    "                            st.markdown(\n",
    "                                f\"\"\"\n",
    "                                <div class=\"hit-card\">\n",
    "                                    <div class=\"hit-meta\">{meta_line}</div>\n",
    "                                    <div class=\"hit-snippet\">{snippet}</div>\n",
    "                                </div>\n",
    "                                \"\"\",\n",
    "                                unsafe_allow_html=True,\n",
    "                            )\n",
    "                    else:\n",
    "                        st.info(msg.get(\"empty_message\", \"No relevant answers found in the approved library.\"))\n",
    "                    continue\n",
    "                st.markdown(msg[\"content\"])\n",
    "                if msg.get(\"model\"):\n",
    "                    name = MODEL_SHORT_NAMES.get(msg[\"model\"], msg[\"model\"]) if view_mode == \"User\" else msg[\"model\"]\n",
    "                    st.caption(f\"Model: {name}\")\n",
    "                if view_mode == \"Developer\" and msg.get(\"debug\"):\n",
    "                    st.expander(\"Debug info\").markdown(f\"```\\n{msg['debug']}\\n```\")\n",
    "                if msg[\"role\"] == \"assistant\" and \"hits\" not in msg:\n",
    "                    answer_idx += 1\n",
    "                    sidebar.markdown(f\"**Answer {answer_idx}**\")\n",
    "                    for lbl, cite in (msg.get(\"citations\") or {}).items():\n",
    "                        with sidebar.expander(str(lbl)):\n",
    "                            st.markdown(f\"**Document:** {cite.get('source_file', 'Unknown')}**\")\n",
    "                            section = cite.get(\"section\")\n",
    "                            if section:\n",
    "                                st.markdown(f\"**Section:** {section}**\")\n",
    "                            st.markdown(cite.get(\"text\", \"\"))\n",
    "        if prompt := st.chat_input(\"Ask a question\"):\n",
    "            st.chat_message(\"user\").markdown(prompt)\n",
    "            st.session_state.chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            history = list(st.session_state.get(\"question_history\", []))\n",
    "            if response_mode == \"Closest pre-approved answers\":\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    container = st.container()\n",
    "                    status_placeholder = container.empty()\n",
    "                    def update_status(msg: str) -> None:\n",
    "                        status_placeholder.markdown(f'<span class=\"shimmer\">{msg}</span>', unsafe_allow_html=True)\n",
    "                    update_status(\"Finding closest pre-approved answers...\")\n",
    "                    rows = collect_relevant_snippets(\n",
    "                        q=prompt,\n",
    "                        mode=search_mode,\n",
    "                        fund=fund,\n",
    "                        k=int(k_max_hits),\n",
    "                        min_confidence=float(min_confidence),\n",
    "                        llm=llm,\n",
    "                        extra_docs=extra_docs,\n",
    "                        progress=update_status,\n",
    "                    )\n",
    "                    status_placeholder.empty()\n",
    "                    hits_payload = []\n",
    "                    if rows:\n",
    "                        for i, (lbl, src_name, snippet, score, date_str) in enumerate(rows, 1):\n",
    "                            hits_payload.append(\n",
    "                                {\n",
    "                                    \"label\": lbl,\n",
    "                                    \"source\": src_name,\n",
    "                                    \"snippet\": snippet,\n",
    "                                    \"score\": score,\n",
    "                                    \"date\": date_str,\n",
    "                                    \"original_index\": i,\n",
    "                                }\n",
    "                            )\n",
    "                        hits_to_show = select_top_preapproved_answers(prompt, hits_payload)\n",
    "                        container.markdown(\"**Closest pre-approved answers**\")\n",
    "                        for display_idx, hit in enumerate(hits_to_show, 1):\n",
    "                            snippet_html = html.escape(hit.get(\"snippet\", \"\"))\n",
    "                            source_html = html.escape(hit.get(\"source\") or \"Unknown\")\n",
    "                            meta_parts = [f\"<strong>{display_idx}. {source_html}</strong>\"]\n",
    "                            score = hit.get(\"score\")\n",
    "                            if isinstance(score, (int, float)):\n",
    "                                meta_parts.append(f\"Score {score:.3f}\")\n",
    "                            elif score:\n",
    "                                meta_parts.append(f\"Score {html.escape(str(score))}\")\n",
    "                            date_str = hit.get(\"date\")\n",
    "                            if date_str:\n",
    "                                meta_parts.append(html.escape(str(date_str)))\n",
    "                            original_idx = hit.get(\"original_index\")\n",
    "                            if original_idx and original_idx != display_idx:\n",
    "                                meta_parts.append(f\"Orig #{original_idx}\")\n",
    "                            meta_line = \" · \".join(meta_parts)\n",
    "                            hit[\"rank\"] = display_idx\n",
    "                            hit[\"selected_by_model\"] = \"o3-2025-04-16_research\"\n",
    "                            container.markdown(\n",
    "                                f\"\"\"\n",
    "                                <div class=\"hit-card\">\n",
    "                                    <div class=\"hit-meta\">{meta_line}</div>\n",
    "                                    <div class=\"hit-snippet\">{snippet_html}</div>\n",
    "                                </div>\n",
    "                                \"\"\",\n",
    "                                unsafe_allow_html=True,\n",
    "                            )\n",
    "                        msg = {\"role\": \"assistant\", \"content\": \"**Closest pre-approved answers**\", \"hits\": hits_to_show}\n",
    "                    else:\n",
    "                        empty_message = \"No relevant answers found in the approved library.\"\n",
    "                        container.info(empty_message)\n",
    "                        msg = {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": \"Closest pre-approved answers\",\n",
    "                            \"hits\": [],\n",
    "                            \"empty_message\": empty_message,\n",
    "                        }\n",
    "            else:\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    message_placeholder = st.empty()\n",
    "                    def update_status(msg: str) -> None:\n",
    "                        message_placeholder.markdown(f'<span class=\"shimmer\">{msg}</span>', unsafe_allow_html=True)\n",
    "                    update_status(\"Thinking...\")\n",
    "                    intent = _classify_intent(prompt, history)\n",
    "                    follow = _detect_followup(prompt, history) if intent == \"follow_up\" else []\n",
    "                    buf = io.StringIO() if view_mode == \"Developer\" else None\n",
    "                    response_model = llm_model\n",
    "                    restore_client = None\n",
    "                    call_fn = gen_answer if intent == \"follow_up\" else gen\n",
    "                    try:\n",
    "                        if intent == \"follow_up\" and view_mode != \"Developer\":\n",
    "                            response_model = FOLLOWUP_DEFAULT_MODEL\n",
    "                            followup_llm = (\n",
    "                                llm\n",
    "                                if response_model == llm_model\n",
    "                                else (\n",
    "                                    CompletionsClient(model=response_model)\n",
    "                                    if framework == \"aladdin\"\n",
    "                                    else OpenAIClient(model=response_model)\n",
    "                                )\n",
    "                            )\n",
    "                            restore_client = my_module._llm_client\n",
    "                            my_module._llm_client = followup_llm\n",
    "                        if buf:\n",
    "                            with contextlib.redirect_stdout(buf):\n",
    "                                ans = call_fn(prompt, progress=update_status)\n",
    "                        else:\n",
    "                            ans = call_fn(prompt, progress=update_status)\n",
    "                    finally:\n",
    "                        if restore_client is not None:\n",
    "                            my_module._llm_client = restore_client\n",
    "                    debug_text = (\n",
    "                        f\"Intent: {intent}\",\n",
    "                        f\"Follow-up indices: {follow}\",\n",
    "                        f\"{buf.getvalue()}\"\n",
    "                        if buf\n",
    "                        else \"\"\n",
    "                    )\n",
    "                    text = ans.get(\"text\", \"\") if isinstance(ans, dict) else ans\n",
    "                    citations = ans.get(\"citations\", {}) if isinstance(ans, dict) else {}\n",
    "                    message_placeholder.markdown(text)\n",
    "                    label = MODEL_SHORT_NAMES.get(response_model, response_model) if view_mode == \"User\" else response_model\n",
    "                    st.caption(f\"Model: {label}\")\n",
    "                    if view_mode == \"Developer\":\n",
    "                        st.expander(\"Debug info\").markdown(f\"```\\n{debug_text}\\n```\")\n",
    "                    if intent != \"follow_up\":\n",
    "                        my_module.QUESTION_HISTORY.append(prompt)\n",
    "                        my_module.QA_HISTORY.append({\"question\": prompt, \"answer\": text, \"citations\": []})\n",
    "                msg = {\"role\": \"assistant\", \"content\": text, \"citations\": citations, \"model\": response_model}\n",
    "                if view_mode == \"Developer\":\n",
    "                    msg[\"debug\"] = debug_text\n",
    "            st.session_state.chat_messages.append(msg)\n",
    "            history.append(prompt)\n",
    "            st.session_state.question_history = history\n",
    "            st.rerun()\n",
    "    else:\n",
    "        run_clicked = st.button(\"Run\")\n",
    "        if run_clicked and uploaded is not None and fund:\n",
    "            phase_placeholder = st.empty()\n",
    "            sub_placeholder = st.empty()\n",
    "            dev_placeholder = st.empty()\n",
    "            dev_logs = []\n",
    "            state = {\"step\": 0, \"phase\": None}\n",
    "            suffix = Path(uploaded.name).suffix.lower()\n",
    "            base_steps = 1\n",
    "            if suffix in (\".xlsx\", \".xls\"):\n",
    "                branch_steps = 4\n",
    "            elif suffix == \".docx\" and not docx_as_text:\n",
    "                branch_steps = 3\n",
    "            else:\n",
    "                branch_steps = 3\n",
    "            total_steps = base_steps + branch_steps + 1\n",
    "            step_bar = st.progress(0)\n",
    "            def log_step(dev_msg, user_msg=None):\n",
    "                if user_msg and user_msg != state[\"phase\"]:\n",
    "                    state[\"phase\"] = user_msg\n",
    "                    phase_placeholder.markdown(f\"**{state['phase']}**\")\n",
    "                    sub_placeholder.empty()\n",
    "                sub_placeholder.markdown(dev_msg)\n",
    "                if view_mode == \"Developer\":\n",
    "                    dev_logs.append(f\"{state['phase']}: {dev_msg}\")\n",
    "                    dev_placeholder.markdown(\"\\n\".join(f\"{i+1}. {m}\" for i, m in enumerate(dev_logs)))\n",
    "                state[\"step\"] += 1\n",
    "                step_bar.progress(state[\"step\"] / total_steps, text=state[\"phase\"])\n",
    "            log_step(\"Saving uploaded file\", \"Preparing document...\")\n",
    "            input_path = save_uploaded_file(uploaded)\n",
    "            extra_docs = [save_uploaded_file(f) for f in extra_uploads] if extra_uploads else None\n",
    "            llm = CompletionsClient(model=llm_model) if framework == \"aladdin\" else OpenAIClient(model=llm_model)\n",
    "            if suffix in (\".xlsx\", \".xls\"):\n",
    "                log_step(\"Collecting non-empty cells\", \"Reading workbook...\")\n",
    "                cells = collect_non_empty_cells(input_path)\n",
    "                log_step(\"Inferring sheet schema\", \"Analyzing workbook structure...\")\n",
    "                schema = ask_sheet_schema(input_path)\n",
    "                log_step(\"Building answer generator\", \"Preparing answer generator...\")\n",
    "                gen = build_generator(\n",
    "                    search_mode,\n",
    "                    fund,\n",
    "                    int(k_max_hits),\n",
    "                    length_opt,\n",
    "                    int(approx_words) if approx_words else None,\n",
    "                    float(min_confidence),\n",
    "                    include_citations,\n",
    "                    llm,\n",
    "                    extra_docs,\n",
    "                )\n",
    "                log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                answers = []\n",
    "                total_qs = len(schema)\n",
    "                progress = st.progress(0)\n",
    "                qa_box = st.container() if show_live else None\n",
    "                for i, entry in enumerate(schema, 1):\n",
    "                    question = (entry.get(\"question_text\") or \"\").strip()\n",
    "                    if show_live and question:\n",
    "                        qa_box.markdown(f\"**Q{i}:** {question}\")\n",
    "                    ans = gen(question)\n",
    "                    answers.append(ans)\n",
    "                    progress.progress(i / total_qs, text=f\"{i}/{total_qs}\")\n",
    "                    if show_live:\n",
    "                        text = ans.get('text', '') if isinstance(ans, dict) else ans\n",
    "                        qa_box.markdown(f\"**A{i}:** {text}\")\n",
    "                out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\")\n",
    "                write_excel_answers(\n",
    "                    schema,\n",
    "                    answers,\n",
    "                    input_path,\n",
    "                    out_tmp.name,\n",
    "                    include_comments=include_citations,\n",
    "                )\n",
    "                with open(out_tmp.name, \"rb\") as f:\n",
    "                    st.download_button(\n",
    "                        \"Download answered workbook\",\n",
    "                        f,\n",
    "                        file_name=Path(uploaded.name).stem + \"_answered.xlsx\",\n",
    "                    )\n",
    "                if include_citations:\n",
    "                    base, _ = os.path.splitext(out_tmp.name)\n",
    "                    comments_path = base + \"_comments.docx\"\n",
    "                    if os.path.exists(comments_path):\n",
    "                        with open(comments_path, \"rb\") as f:\n",
    "                            st.download_button(\n",
    "                                \"Download comments DOCX\",\n",
    "                                f,\n",
    "                                file_name=Path(uploaded.name).stem + \"_comments.docx\",\n",
    "                            )\n",
    "            elif suffix == \".docx\" and not docx_as_text:\n",
    "                log_step(\"Extracting slots from DOCX\", \"Analyzing document...\")\n",
    "                slots_payload = extract_slots_from_docx(input_path)\n",
    "                slots_tmp = tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False, suffix=\".json\")\n",
    "                json.dump(slots_payload, slots_tmp)\n",
    "                slots_tmp.flush()\n",
    "                log_step(\"Building answer generator\", \"Preparing answer generator...\")\n",
    "                gen = build_generator(\n",
    "                    search_mode,\n",
    "                    fund,\n",
    "                    int(k_max_hits),\n",
    "                    length_opt,\n",
    "                    int(approx_words) if approx_words else None,\n",
    "                    float(min_confidence),\n",
    "                    include_citations,\n",
    "                    llm,\n",
    "                    extra_docs,\n",
    "                )\n",
    "                log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                answers_dict = {}\n",
    "                slot_list = slots_payload.get('slots', [])\n",
    "                progress = st.progress(0)\n",
    "                qa_box = st.container() if show_live else None\n",
    "                for i, slot in enumerate(slot_list, 1):\n",
    "                    question = (slot.get(\"question_text\") or \"\").strip()\n",
    "                    if show_live and question:\n",
    "                        qa_box.markdown(f\"**Q{i}:** {question}\")\n",
    "                    ans = gen(question)\n",
    "                    answers_dict[slot.get('id', f'slot_{i}')] = ans\n",
    "                    progress.progress(i / len(slot_list), text=f\"{i}/{len(slot_list)}\")\n",
    "                    if show_live:\n",
    "                        text = ans.get('text', '') if isinstance(ans, dict) else ans\n",
    "                        qa_box.markdown(f\"**A{i}:** {text}\")\n",
    "                answers_tmp = tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False, suffix=\".json\")\n",
    "                json.dump({'by_id': answers_dict}, answers_tmp)\n",
    "                answers_tmp.flush()\n",
    "                out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".docx\")\n",
    "                apply_answers_to_docx(\n",
    "                    docx_path=input_path,\n",
    "                    slots_json_path=slots_tmp.name,\n",
    "                    answers_json_path=answers_tmp.name,\n",
    "                    out_path=out_tmp.name,\n",
    "                    mode=docx_write_mode,\n",
    "                    generator=None,\n",
    "                    gen_name=\"streamlit_app:rag_gen\",\n",
    "                )\n",
    "                with open(out_tmp.name, \"rb\") as f:\n",
    "                    st.download_button(\n",
    "                        \"Download answered DOCX\",\n",
    "                        f,\n",
    "                        file_name=Path(uploaded.name).stem + \"_answered.docx\",\n",
    "                    )\n",
    "            else:\n",
    "                log_step(\"Loading input text\", \"Reading document...\")\n",
    "                raw = load_input_text(input_path)\n",
    "                log_step(\"Extracting questions\", \"Finding questions...\")\n",
    "                questions = extract_questions(raw, llm)\n",
    "                log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                answers = []\n",
    "                comments = []\n",
    "                total_qs = len(questions)\n",
    "                progress = st.progress(0)\n",
    "                qa_box = st.container() if show_live else None\n",
    "                for i, q in enumerate(questions, 1):\n",
    "                    if show_live and q:\n",
    "                        qa_box.markdown(f\"**Q{i}:** {q}\")\n",
    "                    ans, cmts = answer_question(\n",
    "                        q,\n",
    "                        search_mode,\n",
    "                        fund,\n",
    "                        int(k_max_hits),\n",
    "                        length_opt,\n",
    "                        int(approx_words) if approx_words else None,\n",
    "                        float(min_confidence),\n",
    "                        llm,\n",
    "                    )\n",
    "                    if not include_citations:\n",
    "                        ans = re.sub(r\"\\[\\d+\\]\", \"\", ans)\n",
    "                        cmts = []\n",
    "                    answers.append(ans)\n",
    "                    comments.append(cmts)\n",
    "                    progress.progress(i / total_qs, text=f\"{i}/{total_qs}\")\n",
    "                    if show_live:\n",
    "                        qa_box.markdown(f\"**A{i}:** {ans}\")\n",
    "                qa_doc = build_docx(\n",
    "                    questions,\n",
    "                    answers,\n",
    "                    comments,\n",
    "                    include_comments=include_citations,\n",
    "                )\n",
    "                out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".docx\")\n",
    "                out_tmp.write(qa_doc)\n",
    "                out_tmp.flush()\n",
    "                with open(out_tmp.name, \"rb\") as f:\n",
    "                    st.download_button(\n",
    "                        \"Download Q/A report\",\n",
    "                        f,\n",
    "                        file_name=Path(uploaded.name).stem + \"_answered.docx\",\n",
    "                    )\n",
    "            step_bar.progress(1.0, text=\"Done\")\n",
    "        elif run_clicked and not fund:\n",
    "            st.warning(\"Please select a fund or strategy before running.\")\n",
    "        elif run_clicked:\n",
    "            st.warning(\"Please upload a document before running.\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
