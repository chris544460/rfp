{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import streamlit as st\n",
    "from design import APP_NAME, StyleCSS, StyleColors, display_aladdin_logos_and_app_title\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=APP_NAME,\n",
    "    page_icon=\"\ud83d\udcc4\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\",\n",
    ")\n",
    "\n",
    "StyleCSS.set_css_styling()\n",
    "StyleCSS.set_plotly_template(\n",
    "    \"aladdin\",\n",
    "    list(StyleColors.DATAVIZ_COLORS),\n",
    "    set_as_default=True,\n",
    ")\n",
    "display_aladdin_logos_and_app_title()\n",
    "\n",
    "@st.cache_resource\n",
    "def install_packages(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "SETUP_VERSION = \"2025-09-azure-feedback\"\n",
    "\n",
    "packages = [\n",
    "    \"certifi\",\n",
    "    \"charset-normalizer\",\n",
    "    \"faiss-cpu\",\n",
    "    \"idna\",\n",
    "    \"numpy\",\n",
    "    \"packaging\",\n",
    "    \"python-dotenv\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"pyarrow\",\n",
    "    \"PyPDF2\",\n",
    "    \"python-docx\",\n",
    "    \"spacy\",\n",
    "    \"azure-storage-blob\",\n",
    "]\n",
    "\n",
    "def ensure_packages() -> None:\n",
    "    if st.session_state.get(\"setup_version\") == SETUP_VERSION:\n",
    "        return\n",
    "    progress_placeholder = st.empty()\n",
    "    total = len(packages)\n",
    "    for i, package in enumerate(packages, start=1):\n",
    "        try:\n",
    "            install_packages(package)\n",
    "        except subprocess.CalledProcessError:\n",
    "            progress_placeholder.empty()\n",
    "            st.error(\"Something went wrong while setting things up. Please try again or contact support.\")\n",
    "            return\n",
    "        percent = int(i / total * 100)\n",
    "        message = f\"Setting up step {i} of {total}...\"\n",
    "        progress_placeholder.info(f\"{message} ({percent}% complete)\")\n",
    "    progress_placeholder.success(\"Setup complete.\")\n",
    "    st.session_state[\"setup_version\"] = SETUP_VERSION\n",
    "    st.toast(\"You're all set! Choose 'Upload document' to load an RFP or 'Ask a question' to chat. Provide any required API keys in the sidebar.\")\n",
    "\n",
    "ensure_packages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e011114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import html\n",
    "import contextlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from cli_streamlit_app import _resolve_concurrency\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Callable, Dict, Any\n",
    "from cli_app import (\n",
    "    load_input_text,\n",
    "    extract_questions,\n",
    "    build_docx,\n",
    ")\n",
    "from qa_core import answer_question, collect_relevant_snippets\n",
    "from answer_composer import CompletionsClient, get_openai_completion\n",
    "from input_file_reader.interpreter_sheet import collect_non_empty_cells\n",
    "from rfp_xlsx_slot_finder import ask_sheet_schema\n",
    "from rfp_xlsx_apply_answers import write_excel_answers\n",
    "from rfp_docx_slot_finder import extract_slots_from_docx\n",
    "from rfp_docx_apply_answers import apply_answers_to_docx\n",
    "import my_module\n",
    "from my_module import _classify_intent, _detect_followup, gen_answer\n",
    "from feedback_storage import build_feedback_store, FeedbackStorageError\n",
    "LOCAL_FEEDBACK_FILE = Path(\"feedback_log.ndjson\")\n",
    "FEEDBACK_FIELDS = [\n",
    "    \"timestamp\",\n",
    "    \"session_id\",\n",
    "    \"user_id\",\n",
    "    \"feedback_source\",\n",
    "    \"feedback_subject\",\n",
    "    \"rating\",\n",
    "    \"highlights\",\n",
    "    \"improvements\",\n",
    "    \"comment\",\n",
    "    \"question\",\n",
    "    \"answer\",\n",
    "    \"context_json\",\n",
    "]\n",
    "CHAT_HIGHLIGHT_OPTIONS = [\n",
    "    \"Accurate and complete\",\n",
    "    \"Actionable guidance\",\n",
    "    \"Clear explanation\",\n",
    "    \"Helpful citations\",\n",
    "    \"Fast response\",\n",
    "]\n",
    "CHAT_IMPROVEMENT_OPTIONS = [\n",
    "    \"Incorrect information\",\n",
    "    \"Missing details\",\n",
    "    \"Not relevant\",\n",
    "    \"Formatting issues\",\n",
    "    \"Slow response\",\n",
    "]\n",
    "DOC_HIGHLIGHT_OPTIONS = [\n",
    "    \"Captured required fields\",\n",
    "    \"Accurate answers\",\n",
    "    \"Helpful citations\",\n",
    "    \"Easy to download\",\n",
    "]\n",
    "DOC_IMPROVEMENT_OPTIONS = [\n",
    "    \"Incorrect answers\",\n",
    "    \"Missing responses\",\n",
    "    \"Formatting issues\",\n",
    "    \"Download problems\",\n",
    "    \"Slow processing\",\n",
    "]\n",
    "try:\n",
    "    feedback_store = build_feedback_store(FEEDBACK_FIELDS, LOCAL_FEEDBACK_FILE)\n",
    "except FeedbackStorageError as exc:\n",
    "    st.error(f\"Feedback storage is unavailable: {exc}\")\n",
    "    st.stop()\n",
    "\n",
    "def get_current_user() -> str:\n",
    "    return st.session_state.get(\"current_user_id\", \"demo_user\")\n",
    "def serialize_list(items: Optional[List[str]]) -> str:\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    return \" | \".join(item.strip() for item in items if item)\n",
    "def log_feedback(record: dict) -> None:\n",
    "    try:\n",
    "        feedback_store.append(record)\n",
    "    except FeedbackStorageError as exc:\n",
    "        st.error(f\"Unable to save feedback: {exc}\")\n",
    "def format_context(context: dict) -> str:\n",
    "    try:\n",
    "        return json.dumps(context, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "def render_chat_feedback_form(\n",
    "    message_index: int,\n",
    "    question: Optional[str],\n",
    "    answer: str,\n",
    "    message_payload: dict,\n",
    ") -> None:\n",
    "    submitted_map = st.session_state.setdefault(\"chat_feedback_submitted\", {})\n",
    "    feedback_key = f\"chat_{message_index}\"\n",
    "    if submitted_map.get(feedback_key):\n",
    "        st.caption(\"Feedback recorded \u2014 thank you!\")\n",
    "        return\n",
    "    with st.expander(\"How was this answer?\", expanded=False):\n",
    "        rating_key = f\"chat_rating_{message_index}\"\n",
    "        rating_choice = st.radio(\n",
    "            \"Overall\",\n",
    "            [\"Helpful\", \"Needs improvement\"],\n",
    "            horizontal=True,\n",
    "            key=rating_key,\n",
    "        )\n",
    "        highlights_key = f\"chat_highlights_{message_index}\"\n",
    "        improvements_key = f\"chat_improvements_{message_index}\"\n",
    "        with st.form(f\"chat_feedback_form_{message_index}\"):\n",
    "            highlights: List[str] = []\n",
    "            improvements: List[str] = []\n",
    "            if rating_choice == \"Helpful\":\n",
    "                st.session_state.pop(improvements_key, None)\n",
    "                highlights = st.multiselect(\n",
    "                    \"Highlights\",\n",
    "                    CHAT_HIGHLIGHT_OPTIONS,\n",
    "                    key=highlights_key,\n",
    "                )\n",
    "            else:\n",
    "                st.session_state.pop(highlights_key, None)\n",
    "                improvements = st.multiselect(\n",
    "                    \"Opportunities\",\n",
    "                    CHAT_IMPROVEMENT_OPTIONS,\n",
    "                    key=improvements_key,\n",
    "                )\n",
    "            comment = st.text_area(\n",
    "                \"Additional comments\",\n",
    "                placeholder=\"What should we know?\",\n",
    "                key=f\"chat_comment_{message_index}\",\n",
    "            )\n",
    "            submitted = st.form_submit_button(\"Submit feedback\", use_container_width=True)\n",
    "            if submitted:\n",
    "                record = {\n",
    "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    \"session_id\": st.session_state.get(\"session_id\", str(uuid4())),\n",
    "                    \"user_id\": get_current_user(),\n",
    "                    \"feedback_source\": \"chat\",\n",
    "                    \"feedback_subject\": f\"assistant_message_{message_index}\",\n",
    "                    \"rating\": \"positive\" if rating_choice == \"Helpful\" else \"needs_improvement\",\n",
    "                    \"highlights\": serialize_list(highlights),\n",
    "                    \"improvements\": serialize_list(improvements),\n",
    "                    \"comment\": comment.strip(),\n",
    "                    \"question\": question or \"\",\n",
    "                    \"answer\": answer,\n",
    "                    \"context_json\": format_context(\n",
    "                        {\n",
    "                            \"chat_history\": st.session_state.get(\"chat_messages\", []),\n",
    "                            \"message_index\": message_index,\n",
    "                            \"message_payload\": message_payload,\n",
    "                        }\n",
    "                    ),\n",
    "                }\n",
    "                log_feedback(record)\n",
    "                submitted_map[feedback_key] = True\n",
    "                st.success(\"Feedback saved \u2014 thank you!\")\n",
    "\n",
    "\n",
    "def _shorten_question_label(text: str) -> str:\n",
    "    cleaned = (text or '').strip()\n",
    "    if not cleaned:\n",
    "        return 'Pending question'\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned[:87] + '...' if len(cleaned) > 90 else cleaned\n",
    "\n",
    "\n",
    "def _create_live_placeholder(container, idx: int, question_text: str):\n",
    "    if container is None:\n",
    "        return None\n",
    "    # Render a simple card-like container using a single-column layout.\n",
    "    # Avoid a top-level dropdown; instead show the question header inline.\n",
    "    col = container.columns(1)[0]\n",
    "    label = _shorten_question_label(question_text)\n",
    "    with col:\n",
    "        st.markdown(f\"**Q{idx + 1}: {label}\")\n",
    "        placeholder = st.empty()\n",
    "        placeholder.info('Waiting for answer...')\n",
    "    return placeholder\n",
    "\n",
    "\n",
    "def _normalize_citation_entry(comment):\n",
    "    def _clean(value):\n",
    "        return str(value).strip() if value not in (None, '') else ''\n",
    "\n",
    "    if not comment:\n",
    "        return []\n",
    "\n",
    "    if isinstance(comment, dict):\n",
    "        if comment and all(str(k).isdigit() for k in comment.keys()):\n",
    "            entries = []\n",
    "            for key in sorted(comment.keys(), key=lambda k: int(str(k)) if str(k).isdigit() else str(k)):\n",
    "                payload = comment[key]\n",
    "                if isinstance(payload, dict):\n",
    "                    payload = dict(payload)\n",
    "                else:\n",
    "                    payload = {\"text\": payload}\n",
    "                payload.setdefault(\"label\", key)\n",
    "                entries.extend(_normalize_citation_entry(payload))\n",
    "            return entries\n",
    "\n",
    "        label = _clean(comment.get('label') or comment.get('id') or comment.get('index') or comment.get('key') or comment.get('citation'))\n",
    "        source = _clean(comment.get('source') or comment.get('source_file') or comment.get('document') or comment.get('name') or comment.get('file'))\n",
    "        page = _clean(comment.get('page') or comment.get('section') or comment.get('location') or comment.get('page_label'))\n",
    "        snippet = _clean(comment.get('snippet') or comment.get('text') or comment.get('content') or comment.get('passage'))\n",
    "        extra = _clean(comment.get('meta') or comment.get('note'))\n",
    "        if not snippet and extra:\n",
    "            snippet = extra\n",
    "        elif snippet and extra:\n",
    "            snippet = f\"{snippet} ({extra})\"\n",
    "        return [(label, source, snippet, page)]\n",
    "\n",
    "    if isinstance(comment, (list, tuple)):\n",
    "        label = _clean(comment[0] if len(comment) > 0 else '')\n",
    "        source = _clean(comment[1] if len(comment) > 1 else '')\n",
    "        snippet = _clean(comment[2] if len(comment) > 2 else '')\n",
    "        page = _clean(comment[4] if len(comment) > 4 else '')\n",
    "        if not page:\n",
    "            page = _clean(comment[3] if len(comment) > 3 else '')\n",
    "        return [(label, source, snippet, page)]\n",
    "\n",
    "    value = _clean(comment)\n",
    "    if not value:\n",
    "        return []\n",
    "    return [('', '', value, '')]\n",
    "\n",
    "\n",
    "def _render_live_answer(placeholder, answer, comments, include_citations: bool) -> None:\n",
    "    if placeholder is None:\n",
    "        return\n",
    "    if isinstance(answer, dict):\n",
    "        ans_text = answer.get('text', '')\n",
    "    else:\n",
    "        ans_text = str(answer or '')\n",
    "    ans_text = ans_text.strip() or '_No answer generated._'\n",
    "\n",
    "    placeholder.empty()\n",
    "    with placeholder.container():\n",
    "        st.markdown(f\"**Answer:** {ans_text}\")\n",
    "        if not include_citations:\n",
    "            return\n",
    "\n",
    "        raw_items = comments if isinstance(comments, (list, tuple)) else [comments]\n",
    "        normalized = []\n",
    "        for item in raw_items or []:\n",
    "            normalized.extend(_normalize_citation_entry(item))\n",
    "\n",
    "        display_lines = []\n",
    "        for label, source, snippet, page in normalized:\n",
    "            parts = []\n",
    "            if label:\n",
    "                parts.append(f\"[{label}]\")\n",
    "            if source:\n",
    "                parts.append(source)\n",
    "            if page:\n",
    "                parts.append(page)\n",
    "            if snippet:\n",
    "                parts.append(' '.join(snippet.split()))\n",
    "            display = ' \u2014 '.join(parts).strip()\n",
    "            if display:\n",
    "                display_lines.append(display)\n",
    "\n",
    "        if display_lines:\n",
    "            with st.expander('Citations', expanded=False):\n",
    "                st.markdown('\n'.join(f\"- {line}\" for line in display_lines))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _is_table_slot(slot: dict) -> bool:\n",
    "    locator = slot.get(\"answer_locator\") or {}\n",
    "    return isinstance(locator, dict) and locator.get(\"type\") == \"table_cell\"\n",
    "\n",
    "\n",
    "def _sanitize_table_answer(answer) -> str:\n",
    "\n",
    "    if isinstance(answer, dict):\n",
    "        text = str(answer.get(\"text\", \"\"))\n",
    "    else:\n",
    "        text = str(answer or \"\")\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "\n",
    "    def _collapse_table_like(line: str) -> str:\n",
    "        working = line.replace(\"\t\", \" | \").strip()\n",
    "        if not working:\n",
    "            return \"\"\n",
    "        if set(working) <= {\"|\", \":\", \"-\", \" \", \"+\", \"=\"}:\n",
    "            return \"\"\n",
    "        if \"|\" in working:\n",
    "            segments = [seg.strip(\" -\") for seg in working.strip(\"|\").split(\"|\")]\n",
    "            segments = [seg for seg in segments if seg and set(seg) != {'-'}]\n",
    "            working = \" \".join(segments)\n",
    "        working = working.lstrip(\"-\u2022*\u2192\u2022\").strip()\n",
    "        return working\n",
    "    \n",
    "    parts = []\n",
    "    for raw_line in text.splitlines():\n",
    "        collapsed = _collapse_table_like(raw_line)\n",
    "        if collapsed:\n",
    "            parts.append(collapsed)\n",
    "    \n",
    "    prose = \" \".join(parts)\n",
    "    prose = re.sub(r\"\\s+\", \" \", prose).strip()\n",
    "    if not prose:\n",
    "        prose = \"No information found.\"\n",
    "    if not prose.endswith(('.', '!', '?')):\n",
    "        prose += '.'\n",
    "    return prose\n",
    "\n",
    "\n",
    "\n",
    "def _reset_doc_downloads() -> None:\n",
    "    st.session_state[\"doc_downloads\"] = {}\n",
    "\n",
    "\n",
    "def _store_doc_download(\n",
    "    key: str,\n",
    "    *,\n",
    "    label: str,\n",
    "    data: bytes,\n",
    "    file_name: str,\n",
    "    mime: Optional[str] = None,\n",
    "    order: int = 0,\n",
    ") -> None:\n",
    "    bucket: Dict[str, Dict[str, Any]] = st.session_state.setdefault(\"doc_downloads\", {})\n",
    "    bucket[key] = {\n",
    "        \"label\": label,\n",
    "        \"data\": data,\n",
    "        \"file_name\": file_name,\n",
    "        \"mime\": mime,\n",
    "        \"order\": order,\n",
    "    }\n",
    "\n",
    "\n",
    "def _render_doc_downloads(target=None) -> None:\n",
    "    downloads: Dict[str, Dict[str, Any]] = st.session_state.get(\"doc_downloads\") or {}\n",
    "    if not downloads:\n",
    "        return\n",
    "    target = target or st\n",
    "    for key, meta in sorted(downloads.items(), key=lambda item: item[1].get(\"order\", 0)):\n",
    "        target.download_button(\n",
    "            meta[\"label\"],\n",
    "            meta[\"data\"],\n",
    "            file_name=meta[\"file_name\"],\n",
    "            mime=meta.get(\"mime\"),\n",
    "            key=f\"doc_download_{key}\",\n",
    "        )\n",
    "\n",
    "def render_document_feedback_section(run_context: Optional[dict]) -> None:\n",
    "    if not run_context:\n",
    "        return\n",
    "    submitted = st.session_state.get(\"doc_feedback_submitted\", False)\n",
    "    expander_label = \"Share feedback on this document run\"\n",
    "    with st.expander(expander_label, expanded=not submitted):\n",
    "        if submitted:\n",
    "            st.caption(\"Feedback recorded \u2014 thank you!\")\n",
    "            return\n",
    "        with st.form(\"document_feedback_form\"):\n",
    "            rating_choice = st.radio(\n",
    "                \"Overall\",\n",
    "                [\"Helpful\", \"Needs improvement\"],\n",
    "                horizontal=True,\n",
    "            )\n",
    "            highlights = st.multiselect(\n",
    "                \"What worked well?\",\n",
    "                DOC_HIGHLIGHT_OPTIONS,\n",
    "            )\n",
    "            improvements = st.multiselect(\n",
    "                \"What should we improve?\",\n",
    "                DOC_IMPROVEMENT_OPTIONS,\n",
    "            )\n",
    "            comment = st.text_area(\"Additional comments\", placeholder=\"Optional details\u2026\")\n",
    "            submitted_form = st.form_submit_button(\"Submit feedback\", use_container_width=True)\n",
    "            if submitted_form:\n",
    "                record = {\n",
    "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    \"session_id\": st.session_state.get(\"session_id\", str(uuid4())),\n",
    "                    \"user_id\": get_current_user(),\n",
    "                    \"feedback_source\": \"document\",\n",
    "                    \"feedback_subject\": run_context.get(\"uploaded_name\", \"document_run\"),\n",
    "                    \"rating\": \"positive\" if rating_choice == \"Helpful\" else \"needs_improvement\",\n",
    "                    \"highlights\": serialize_list(highlights),\n",
    "                    \"improvements\": serialize_list(improvements),\n",
    "                    \"comment\": comment.strip(),\n",
    "                    \"question\": \"\",\n",
    "                    \"answer\": \"\",\n",
    "                    \"context_json\": format_context(run_context),\n",
    "                }\n",
    "                log_feedback(record)\n",
    "                st.session_state.doc_feedback_submitted = True\n",
    "                st.success(\"Feedback saved \u2014 thank you!\")\n",
    "MODEL_DESCRIPTIONS = {\n",
    "    \"gpt-4.1-nano-2025-04-14_research\": \"Lighter, faster model\",\n",
    "    \"o3-2025-04-16_research\": \"Slower, reasoning model\",\n",
    "}\n",
    "MODEL_SHORT_NAMES = {\n",
    "    \"gpt-4.1-nano-2025-04-14_research\": \"4.1\",\n",
    "    \"o3-2025-04-16_research\": \"o3\",\n",
    "}\n",
    "MODEL_OPTIONS = list(MODEL_DESCRIPTIONS.keys())\n",
    "FOLLOWUP_DEFAULT_MODEL = \"gpt-4.1-nano-2025-04-14_research\"\n",
    "DEFAULT_MODEL = \"o3-2025-04-16_research\"\n",
    "DOC_DEFAULT_MODEL = \"o3-2025-04-16_research\"\n",
    "try:\n",
    "    DEFAULT_INDEX = MODEL_OPTIONS.index(DEFAULT_MODEL)\n",
    "except ValueError:\n",
    "    DEFAULT_INDEX = 0\n",
    "    DEFAULT_MODEL = MODEL_OPTIONS[0]\n",
    "def load_fund_tags() -> List[str]:\n",
    "    path = Path('~/derivs-tool/rfp-ai-tool/structured_extraction/embedding_data.json').expanduser()\n",
    "    try:\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception:\n",
    "        return []\n",
    "    tags = {t for item in data for t in item.get('metadata', {}).get('tags', [])}\n",
    "    return sorted(tags)\n",
    "class OpenAIClient:\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "    def get_completion(self, prompt: str, json_output: bool = False):\n",
    "        return get_openai_completion(prompt, self.model, json_output=json_output)\n",
    "def save_uploaded_file(uploaded_file) -> str:\n",
    "    suffix = Path(uploaded_file.name).suffix\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n",
    "    tmp.write(uploaded_file.read())\n",
    "    tmp.flush()\n",
    "    return tmp.name\n",
    "def build_generator(\n",
    "    search_mode: str,\n",
    "    fund: Optional[str],\n",
    "    k: int,\n",
    "    length: Optional[str],\n",
    "    approx_words: Optional[int],\n",
    "    min_confidence: float,\n",
    "    include_citations: bool,\n",
    "    llm,\n",
    "    extra_docs: Optional[List[str]] = None,\n",
    "):\n",
    "    def gen(question: str, progress: Optional[Callable[[str], None]] = None):\n",
    "        ans, cmts = answer_question(\n",
    "            question,\n",
    "            search_mode,\n",
    "            fund,\n",
    "            k,\n",
    "            length,\n",
    "            approx_words,\n",
    "            min_confidence,\n",
    "            llm,\n",
    "            extra_docs=extra_docs,\n",
    "            progress=progress,\n",
    "        )\n",
    "        if not include_citations:\n",
    "            ans = re.sub(r\"\\[\\d+\\]\", \"\", ans)\n",
    "            return ans\n",
    "        citations = {\n",
    "            lbl: {\"text\": snippet, \"source_file\": src}\n",
    "            for lbl, src, snippet, score, date in cmts\n",
    "        }\n",
    "        return {\"text\": ans, \"citations\": citations}\n",
    "    return gen\n",
    "def select_top_preapproved_answers(question: str, hits: List[dict], limit: int = 5) -> List[dict]:\n",
    "    \"\"\"Use the Aladdin completions client to pick the most relevant pre-approved answers.\"\"\"\n",
    "    if len(hits) <= limit:\n",
    "        return hits\n",
    "    formatted = []\n",
    "    for idx, hit in enumerate(hits, 1):\n",
    "        snippet = (hit.get(\"snippet\") or \"\").strip().replace(\"\", \" \")\n",
    "        if len(snippet) > 500:\n",
    "            snippet = snippet[:497] + \"...\"\n",
    "        source = hit.get(\"source\") or \"unknown\"\n",
    "        score = hit.get(\"score\")\n",
    "        if isinstance(score, (int, float)):\n",
    "            score_repr = f\"{score:.3f}\"\n",
    "        else:\n",
    "            score_repr = str(score) if score is not None else \"unknown\"\n",
    "        date = hit.get(\"date\") or \"unknown\"\n",
    "        formatted.append(\n",
    "            f\"{idx}. Source: {source}\\nScore: {score_repr}\\nDate: {date}\\nSnippet: {snippet}\"\n",
    "        )\n",
    "    prompt = (\n",
    "        \"You are ranking pre-approved RFP answers for how well they address a user's question. \"\n",
    "        f\"Return a JSON object with a 'selections' array containing up to {limit} items. \"\n",
    "        \"Each selection must include an 'index' (1-based) pointing to the candidate and a 'reason' in one or two sentences \"\n",
    "        \"explaining how the candidate addresses the user's question.\"\n",
    "        f\"\\n\\nQuestion: {question}\"\n",
    "        \"\\n\\nCandidates:\\n\" + \"\\n\\n\".join(formatted)\n",
    "    )\n",
    "    model_name = os.environ.get(\"ALADDIN_RERANK_MODEL\", \"o3-2025-04-16_research\")\n",
    "    try:\n",
    "        client = CompletionsClient(model=model_name)\n",
    "        content, _ = client.get_completion(prompt, json_output=True)\n",
    "        data = json.loads(content or \"{}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"select_top_preapproved_answers failed with {model_name}: {exc}\")\n",
    "        return hits[:limit]\n",
    "    selected: List[dict] = []\n",
    "    seen = set()\n",
    "    def add_hit(position: int, reason: Optional[str] = None) -> None:\n",
    "        if not isinstance(position, int):\n",
    "            return\n",
    "        if not (1 <= position <= len(hits)):\n",
    "            return\n",
    "        if position in seen:\n",
    "            return\n",
    "        seen.add(position)\n",
    "        hit_data = dict(hits[position - 1])\n",
    "        if reason:\n",
    "            cleaned = \" \".join(str(reason).strip().split())\n",
    "            if cleaned:\n",
    "                hit_data[\"selection_reason\"] = cleaned\n",
    "        hit_data.setdefault(\"selected_by_model\", model_name)\n",
    "        selected.append(hit_data)\n",
    "    selections = (\n",
    "        data.get(\"selections\")\n",
    "        or data.get(\"choices\")\n",
    "        or data.get(\"ranked\")\n",
    "        or data.get(\"results\")\n",
    "        or []\n",
    "    )\n",
    "    if isinstance(selections, dict):\n",
    "        for value in selections.values():\n",
    "            if isinstance(value, list):\n",
    "                selections = value\n",
    "                break\n",
    "    if isinstance(selections, list):\n",
    "        for entry in selections:\n",
    "            if len(selected) == limit:\n",
    "                break\n",
    "            reason = None\n",
    "            idx_value = None\n",
    "            if isinstance(entry, dict):\n",
    "                reason = entry.get(\"reason\") or entry.get(\"rationale\") or entry.get(\"why\")\n",
    "                idx_value = (\n",
    "                    entry.get(\"index\")\n",
    "                    or entry.get(\"idx\")\n",
    "                    or entry.get(\"rank\")\n",
    "                    or entry.get(\"position\")\n",
    "                )\n",
    "            else:\n",
    "                idx_value = entry\n",
    "            try:\n",
    "                pos = int(idx_value)\n",
    "            except (TypeError, ValueError):\n",
    "                continue\n",
    "            add_hit(pos, reason)\n",
    "    if len(selected) < limit:\n",
    "        indices = data.get(\"top_indices\") or data.get(\"top\") or data.get(\"indices\") or []\n",
    "        if isinstance(indices, (list, tuple)):\n",
    "            for idx in indices:\n",
    "                if len(selected) == limit:\n",
    "                    break\n",
    "                try:\n",
    "                    pos = int(idx)\n",
    "                except (TypeError, ValueError):\n",
    "                    continue\n",
    "                add_hit(pos, None)\n",
    "    if len(selected) < limit:\n",
    "        for position in range(1, len(hits) + 1):\n",
    "            if len(selected) == limit:\n",
    "                break\n",
    "            if position in seen:\n",
    "                continue\n",
    "            add_hit(position, None)\n",
    "    if not selected:\n",
    "        return hits[:limit]\n",
    "    return selected[:limit]\n",
    "def main():\n",
    "    st.title(\"RFP Responder\")\n",
    "    if \"session_id\" not in st.session_state:\n",
    "        st.session_state.session_id = str(uuid4())\n",
    "    st.session_state.setdefault(\"chat_feedback_submitted\", {})\n",
    "    st.session_state.setdefault(\"doc_feedback_submitted\", False)\n",
    "    st.session_state.setdefault(\"latest_doc_run\", None)\n",
    "    st.session_state.setdefault(\"doc_processing_state\", \"idle\")\n",
    "    st.session_state.setdefault(\"doc_processing_result\", None)\n",
    "    st.session_state.setdefault(\"doc_processing_error\", None)\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        div.block-container{\n",
    "            max-width: 900px;\n",
    "            padding: 32px 48px;\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage\"]{\n",
    "            border-radius: 0;\n",
    "            border: 1px solid #d4d6db;\n",
    "            padding: 16px;\n",
    "            margin-bottom: 16px;\n",
    "            box-shadow: 0 2px 6px rgba(17, 24, 39, 0.08);\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage-user\"]{\n",
    "            background-color: #f2f5f8;\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage-assistant\"]{\n",
    "            background-color: #ffffff;\n",
    "        }\n",
    "        div[data-testid=\"stChatInput\"] textarea{\n",
    "            border-radius: 0;\n",
    "            border: 1px solid #d4d6db;\n",
    "            padding: 12px;\n",
    "        }\n",
    "        @keyframes shimmer{\n",
    "            0%{background-position:-1000px 0;}\n",
    "            100%{background-position:1000px 0;}\n",
    "        }\n",
    "        .shimmer{\n",
    "            background:linear-gradient(90deg,#d0d0d0 0%,#b0b0b0 50%,#d0d0d0 100%);\n",
    "            background-size:1000px 100%;\n",
    "            animation:shimmer 2s infinite linear;\n",
    "            -webkit-background-clip:text;\n",
    "            -webkit-text-fill-color:transparent;\n",
    "        }\n",
    "        .hit-card{\n",
    "            border:1px solid #d4d6db;\n",
    "            border-radius:0;\n",
    "            padding:18px 22px;\n",
    "            background-color:#f5f6f8;\n",
    "            margin-top:18px;\n",
    "            box-shadow: 0 2px 6px rgba(17, 24, 39, 0.08);\n",
    "        }\n",
    "        .hit-meta{\n",
    "            font-size:0.85rem;\n",
    "            color:#525760;\n",
    "            margin-bottom:12px;\n",
    "        }\n",
    "        .hit-reason{\n",
    "            font-size:0.9rem;\n",
    "            color:#30343b;\n",
    "            margin-bottom:12px;\n",
    "        }\n",
    "        .hit-reason-label{\n",
    "            font-weight:600;\n",
    "            color:#1f232a;\n",
    "            margin-right:6px;\n",
    "        }\n",
    "        .hit-snippet{\n",
    "            font-size:0.95rem;\n",
    "            line-height:1.6;\n",
    "            color:#111827;\n",
    "        }\n",
    "        .hit-score{\n",
    "            font-size:0.8rem;\n",
    "            color:#6b7280;\n",
    "            margin-top:6px;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "    view_mode = \"User\"\n",
    "    input_mode = st.radio(\"How would you like to proceed?\", [\"Upload document\", \"Ask a question\"], index=1, horizontal=True)\n",
    "    doc_default_model = DOC_DEFAULT_MODEL if DOC_DEFAULT_MODEL in MODEL_OPTIONS else MODEL_OPTIONS[DEFAULT_INDEX]\n",
    "    llm_model = doc_default_model\n",
    "    framework_env = os.getenv(\"ANSWER_FRAMEWORK\")\n",
    "    if framework_env:\n",
    "        if view_mode == \"Developer\":\n",
    "            st.info(f\"Using framework from ANSWER_FRAMEWORK: {framework_env}\")\n",
    "        framework = framework_env\n",
    "    else:\n",
    "        framework = st.selectbox(\"Framework\", [\"aladdin\", \"openai\"], index=0, help=\"Choose backend for language model.\")\n",
    "    if framework == \"aladdin\":\n",
    "        for key, label in [\n",
    "            (\"aladdin_studio_api_key\", \"Aladdin Studio API key\"),\n",
    "            (\"defaultWebServer\", \"Default Web Server\"),\n",
    "            (\"aladdin_user\", \"Aladdin user\"),\n",
    "            (\"aladdin_passwd\", \"Aladdin password\"),\n",
    "        ]:\n",
    "            if os.getenv(key):\n",
    "                if view_mode == \"Developer\":\n",
    "                    st.info(f\"{key} loaded from environment\")\n",
    "            else:\n",
    "                val = st.text_input(label, type=\"password\" if \"passwd\" in key or \"api_key\" in key else \"default\")\n",
    "                if val:\n",
    "                    os.environ[key] = val\n",
    "    else:\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            if view_mode == \"Developer\":\n",
    "                st.info(\"OPENAI_API_KEY loaded from environment\")\n",
    "        else:\n",
    "            api_key = st.text_input(\"OpenAI API key\", type=\"password\", help=\"API key for OpenAI.\")\n",
    "            if api_key:\n",
    "                os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    if input_mode == \"Upload document\":\n",
    "        uploaded = st.file_uploader(\n",
    "            \"Upload document\",\n",
    "            type=[\"pdf\", \"docx\", \"xls\", \"xlsx\"],\n",
    "            help=\"Upload the RFP or question file (PDF, Word, or Excel).\",\n",
    "        )\n",
    "        if uploaded and Path(uploaded.name).suffix.lower() not in [\".pdf\", \".docx\", \".xls\", \".xlsx\"]:\n",
    "            st.warning(\"Only PDF, Word, or Excel documents are supported.\")\n",
    "            uploaded = None\n",
    "    else:\n",
    "        uploaded = None\n",
    "    if view_mode == \"Developer\":\n",
    "        st.info(\"Search mode fixed to 'both'\")\n",
    "        search_mode = \"both\"\n",
    "        fund = st.selectbox(\n",
    "            \"Fund\", [\"\"] + load_fund_tags(), index=0,\n",
    "            help=\"Filter answers for a specific fund or strategy.\",\n",
    "        )\n",
    "        llm_model = st.selectbox(\n",
    "            \"LLM model\",\n",
    "            MODEL_OPTIONS,\n",
    "            index=DEFAULT_INDEX,\n",
    "            format_func=lambda m: f\"{m} - {MODEL_DESCRIPTIONS[m]}\",\n",
    "            help=\"Model name for generating answers.\",\n",
    "        )\n",
    "        k_max_hits = st.number_input(\"Hits per question\", value=10, help=\"Maximum documents retrieved per question.\")\n",
    "        min_confidence = st.number_input(\"Min confidence\", value=0.0, help=\"Minimum score for retrieved documents.\")\n",
    "        docx_as_text = st.checkbox(\"Treat DOCX as text\", value=False)\n",
    "        docx_write_mode = st.selectbox(\"DOCX write mode\", [\"fill\", \"replace\", \"append\"], index=0)\n",
    "        extra_uploads = st.file_uploader(\n",
    "            \"Additional documents\",\n",
    "            type=[\"pdf\", \"docx\", \"xls\", \"xlsx\"],\n",
    "            accept_multiple_files=True,\n",
    "            help=\"Additional PDF or Word documents to include in search.\",\n",
    "        )\n",
    "        if extra_uploads:\n",
    "            valid_files = []\n",
    "            invalid_files = []\n",
    "            for f in extra_uploads:\n",
    "                if Path(f.name).suffix.lower() in [\".pdf\", \".docx\", \".xls\", \".xlsx\"]:\n",
    "                    valid_files.append(f)\n",
    "                else:\n",
    "                    invalid_files.append(f.name)\n",
    "            if invalid_files:\n",
    "                st.warning(\"Unsupported file types were ignored: \" + \", \".join(invalid_files))\n",
    "            extra_uploads = valid_files\n",
    "    else:\n",
    "        search_mode = \"both\"\n",
    "        fund = st.selectbox(\n",
    "            \"Fund\", [\"\"] + load_fund_tags(), index=0,\n",
    "            help=\"Select fund or strategy context for better answers.\",\n",
    "        )\n",
    "        k_max_hits = 20\n",
    "        min_confidence = 0.0\n",
    "        docx_as_text = False\n",
    "        docx_write_mode = \"fill\"\n",
    "        extra_uploads = st.file_uploader(\n",
    "            \"Additional documents\",\n",
    "            type=[\"pdf\", \"docx\", \"xls\", \"xlsx\"],\n",
    "            accept_multiple_files=True,\n",
    "            help=\"Additional PDF or Word documents to include in search.\",\n",
    "        )\n",
    "        if extra_uploads:\n",
    "            valid_files = []\n",
    "            invalid_files = []\n",
    "            for f in extra_uploads:\n",
    "                if Path(f.name).suffix.lower() in [\".pdf\", \".docx\", \".xls\", \".xlsx\"]:\n",
    "                    valid_files.append(f)\n",
    "                else:\n",
    "                    invalid_files.append(f.name)\n",
    "            if invalid_files:\n",
    "                st.warning(\"Unsupported file types were ignored: \" + \", \".join(invalid_files))\n",
    "            extra_uploads = valid_files\n",
    "    with st.expander(\"More options\"):\n",
    "        if view_mode == \"User\":\n",
    "            llm_model = st.selectbox(\n",
    "                \"Model\",\n",
    "                MODEL_OPTIONS,\n",
    "                index=MODEL_OPTIONS.index(llm_model),\n",
    "                format_func=lambda m: f\"{MODEL_SHORT_NAMES[m]} - {MODEL_DESCRIPTIONS[m]}\",\n",
    "                help=\"Choose which model generates answers.\",\n",
    "            )\n",
    "        length_opt = st.selectbox(\"Answer length\", [\"auto\", \"short\", \"medium\", \"long\"], index=3)\n",
    "        approx_words = st.text_input(\"Approx words\", value=\"\", help=\"Approximate words per answer (optional).\")\n",
    "        include_env = os.getenv(\"RFP_INCLUDE_COMMENTS\")\n",
    "        if include_env is not None:\n",
    "            include_citations = include_env != \"0\"\n",
    "            st.info(f\"Using include citations from RFP_INCLUDE_COMMENTS: {include_citations}\")\n",
    "        else:\n",
    "            include_citations = st.checkbox(\"Include citations\", value=True)\n",
    "        show_live = st.checkbox(\"Show questions and answers during processing\", value=True)\n",
    "    if input_mode == \"Ask a question\":\n",
    "        extra_docs = [save_uploaded_file(f) for f in extra_uploads] if extra_uploads else None\n",
    "        llm = CompletionsClient(model=llm_model) if framework == \"aladdin\" else OpenAIClient(model=llm_model)\n",
    "        gen = build_generator(\n",
    "            search_mode,\n",
    "            fund,\n",
    "            int(k_max_hits),\n",
    "            length_opt,\n",
    "            int(approx_words) if approx_words else None,\n",
    "            float(min_confidence),\n",
    "            include_citations,\n",
    "            llm,\n",
    "            extra_docs,\n",
    "        )\n",
    "        my_module._llm_client = llm\n",
    "        response_mode = st.radio(\n",
    "            \"Response style\",\n",
    "            [\"Generate answer\", \"Closest pre-approved answers\"],\n",
    "            index=0,\n",
    "            horizontal=True,\n",
    "            help=\"Switch between generating an answer or browsing the closest approved responses.\",\n",
    "        )\n",
    "        if \"chat_messages\" not in st.session_state:\n",
    "            st.session_state.chat_messages = []\n",
    "        if \"question_history\" not in st.session_state:\n",
    "            st.session_state.question_history = []\n",
    "        sidebar = st.sidebar.container()\n",
    "        sidebar.markdown(\"### References\")\n",
    "        answer_idx = 0\n",
    "        last_user_message = None\n",
    "        for idx, msg in enumerate(st.session_state.chat_messages):\n",
    "            with st.chat_message(msg[\"role\"]):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    st.markdown(msg.get(\"content\", \"\"))\n",
    "                    last_user_message = msg.get(\"content\", \"\")\n",
    "                    continue\n",
    "                if \"hits\" in msg:\n",
    "                    st.markdown(msg.get(\"content\", \"\"))\n",
    "                    hits = msg.get(\"hits\") or []\n",
    "                    answer_summary = msg.get(\"content\", \"\")\n",
    "                    if hits:\n",
    "                        summary_lines = []\n",
    "                        for i, hit in enumerate(hits, 1):\n",
    "                            snippet = html.escape(hit.get(\"snippet\", \"\"))\n",
    "                            source_name = html.escape(hit.get(\"source\", \"Unknown\"))\n",
    "                            meta_parts = [f\"<strong>{i}. {source_name}</strong>\"]\n",
    "                            score_val = hit.get(\"score\")\n",
    "                            if isinstance(score_val, (int, float)):\n",
    "                                meta_parts.append(f\"Score {score_val:.3f}\")\n",
    "                            elif score_val:\n",
    "                                meta_parts.append(f\"Score {html.escape(str(score_val))}\")\n",
    "                            date_val = hit.get(\"date\")\n",
    "                            if date_val:\n",
    "                                meta_parts.append(html.escape(str(date_val)))\n",
    "                            meta_line = \" \u00b7 \".join(meta_parts)\n",
    "                            reason_text = hit.get(\"selection_reason\")\n",
    "                            reason_block = \"\"\n",
    "                            if reason_text:\n",
    "                                cleaned_reason = \" \".join(str(reason_text).strip().split())\n",
    "                                if cleaned_reason:\n",
    "                                    reason_block = (\n",
    "                                        f'<div class=\"hit-reason\"><span class=\"hit-reason-label\">Model rationale:</span>'\n",
    "                                        f\"{html.escape(cleaned_reason)}</div>\"\n",
    "                                    )\n",
    "                            st.markdown(\n",
    "                                f\"\"\"\n",
    "                                <div class=\"hit-card\">\n",
    "                                    <div class=\"hit-meta\">{meta_line}</div>\n",
    "                                    {reason_block}\n",
    "                                    <div class=\"hit-snippet\">{snippet}</div>\n",
    "                                </div>\n",
    "                                \"\"\",\n",
    "                                unsafe_allow_html=True,\n",
    "                            )\n",
    "                            snippet_plain = (hit.get(\"snippet\") or \"\").strip()\n",
    "                            if snippet_plain:\n",
    "                                summary_lines.append(f\"{i}. {snippet_plain}\")\n",
    "                    else:\n",
    "                        st.info(msg.get(\"empty_message\", \"No relevant answers found in the approved library.\"))\n",
    "                    if hits:\n",
    "                        joined = \"\\n\".join(summary_lines)\n",
    "                        if joined:\n",
    "                            answer_summary = f\"{answer_summary}\\n{joined}\".strip()\n",
    "                    render_chat_feedback_form(\n",
    "                        message_index=idx,\n",
    "                        question=last_user_message,\n",
    "                        answer=answer_summary,\n",
    "                        message_payload=msg,\n",
    "                    )\n",
    "                    continue\n",
    "                st.markdown(msg.get(\"content\", \"\"))\n",
    "                if msg.get(\"model\"):\n",
    "                    name = MODEL_SHORT_NAMES.get(msg[\"model\"], msg[\"model\"]) if view_mode == \"User\" else msg[\"model\"]\n",
    "                    st.caption(f\"Model: {name}\")\n",
    "                if view_mode == \"Developer\" and msg.get(\"debug\"):\n",
    "                    st.expander(\"Debug info\").markdown(f\"```\\n{msg['debug']}\\n```\")\n",
    "                if msg[\"role\"] == \"assistant\" and \"hits\" not in msg:\n",
    "                    answer_idx += 1\n",
    "                    sidebar.markdown(f\"**Answer {answer_idx}**\")\n",
    "                    citations_map = msg.get(\"citations\") or {}\n",
    "                    if citations_map:\n",
    "                        for lbl, cite in citations_map.items():\n",
    "                            source_name = (cite.get('source_file') or 'Unknown').strip() or 'Unknown'\n",
    "                            exp_label = f\"[{lbl}] {source_name}\"\n",
    "                            with sidebar.expander(exp_label):\n",
    "                                section_value = (cite.get('section') or '').strip()\n",
    "                                if section_value:\n",
    "                                    st.markdown(f\"**Section:** {section_value}**\")\n",
    "                                snippet_text = (cite.get('text') or '').strip()\n",
    "                                if snippet_text:\n",
    "                                    st.markdown(snippet_text)\n",
    "                                else:\n",
    "                                    st.caption(\"Snippet not available.\")\n",
    "                    else:\n",
    "                        sidebar.caption(\"No source details returned.\")\n",
    "                    render_chat_feedback_form(\n",
    "                        message_index=idx,\n",
    "                        question=last_user_message,\n",
    "                        answer=msg.get(\"content\", \"\"),\n",
    "                        message_payload=msg,\n",
    "                    )\n",
    "        if prompt := st.chat_input(\"Ask a question\"):\n",
    "            st.chat_message(\"user\").markdown(prompt)\n",
    "            st.session_state.chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            history = list(st.session_state.get(\"question_history\", []))\n",
    "            if response_mode == \"Closest pre-approved answers\":\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    container = st.container()\n",
    "                    status_placeholder = container.empty()\n",
    "                    def _format_preapproved_status(raw: str) -> str:\n",
    "                        text = (raw or \"\").strip()\n",
    "                        lower = text.lower()\n",
    "                        if \"search\" in lower:\n",
    "                            return \"Stage 1/3 - Searching approved library...\"\n",
    "                        if \"candidate\" in lower or \"filter\" in lower:\n",
    "                            return \"Stage 2/3 - Filtering matches...\"\n",
    "                        if any(token in lower for token in (\"complete\", \"finished\", \"done\", \"ready\")):\n",
    "                            return \"Stage 3/3 - Finalizing results...\"\n",
    "                        return text or \"Working...\"\n",
    "\n",
    "                    def _set_preapproved_status(message: str, final: bool = False) -> None:\n",
    "                        display = message or \"Working...\"\n",
    "                        if final:\n",
    "                            status_placeholder.success(display)\n",
    "                        else:\n",
    "                            status_placeholder.markdown(\n",
    "                                f'<span class=\"shimmer\">{display}</span>',\n",
    "                                unsafe_allow_html=True,\n",
    "                            )\n",
    "\n",
    "                    def update_status(msg: str) -> None:\n",
    "                        formatted = _format_preapproved_status(msg)\n",
    "                        lower = (msg or \"\").lower()\n",
    "                        final = any(token in lower for token in (\"complete\", \"finished\", \"done\", \"ready\"))\n",
    "                        _set_preapproved_status(formatted, final=final)\n",
    "\n",
    "                    _set_preapproved_status(\"Stage 1/3 - Searching approved library...\")\n",
    "                    rows = collect_relevant_snippets(\n",
    "                        q=prompt,\n",
    "                        mode=search_mode,\n",
    "                        fund=fund,\n",
    "                        k=int(k_max_hits),\n",
    "                        min_confidence=float(min_confidence),\n",
    "                        llm=llm,\n",
    "                        extra_docs=extra_docs,\n",
    "                        progress=update_status,\n",
    "                    )\n",
    "                    _set_preapproved_status(\"Stage 3/3 - Results ready.\", final=True)\n",
    "                    hits_payload = []\n",
    "                    ranking_placeholder = container.empty()\n",
    "                    if rows:\n",
    "                        ranking_placeholder.markdown(\n",
    "                            '<span class=\"shimmer\">Composing ranked answers...</span>',\n",
    "                            unsafe_allow_html=True,\n",
    "                        )\n",
    "                        for i, (lbl, src_name, snippet, score, date_str) in enumerate(rows, 1):\n",
    "                            hits_payload.append(\n",
    "                                {\n",
    "                                    \"label\": lbl,\n",
    "                                    \"source\": src_name,\n",
    "                                    \"snippet\": snippet,\n",
    "                                    \"score\": score,\n",
    "                                    \"date\": date_str,\n",
    "                                    \"original_index\": i,\n",
    "                                }\n",
    "                            )\n",
    "                        hits_to_show = select_top_preapproved_answers(prompt, hits_payload)\n",
    "                        ranking_placeholder.empty()\n",
    "                        container.markdown(\"**Closest pre-approved answers**\")\n",
    "                        model_used = os.environ.get(\"ALADDIN_RERANK_MODEL\", \"o3-2025-04-16_research\")\n",
    "                        summary_lines = []\n",
    "                        for display_idx, hit in enumerate(hits_to_show, 1):\n",
    "                            snippet_html = html.escape(hit.get(\"snippet\", \"\"))\n",
    "                            snippet_plain = (hit.get(\"snippet\") or \"\").strip()\n",
    "                            if snippet_plain:\n",
    "                                summary_lines.append(f\"{display_idx}. {snippet_plain}\")\n",
    "                            source_html = html.escape(hit.get(\"source\") or \"Unknown\")\n",
    "                            meta_parts = [f\"<strong>{display_idx}. {source_html}</strong>\"]\n",
    "                            score = hit.get(\"score\")\n",
    "                            if isinstance(score, (int, float)):\n",
    "                                meta_parts.append(f\"Score {score:.3f}\")\n",
    "                            elif score:\n",
    "                                meta_parts.append(f\"Score {html.escape(str(score))}\")\n",
    "                            date_str = hit.get(\"date\")\n",
    "                            if date_str:\n",
    "                                meta_parts.append(html.escape(str(date_str)))\n",
    "                            original_idx = hit.get(\"original_index\")\n",
    "                            if original_idx and original_idx != display_idx:\n",
    "                                meta_parts.append(f\"Orig #{original_idx}\")\n",
    "                            meta_line = \" \u00b7 \".join(meta_parts)\n",
    "                            hit[\"rank\"] = display_idx\n",
    "                            hit.setdefault(\"selected_by_model\", model_used)\n",
    "                            reason_text = hit.get(\"selection_reason\")\n",
    "                            reason_block = \"\"\n",
    "                            if reason_text:\n",
    "                                cleaned_reason = \" \".join(str(reason_text).strip().split())\n",
    "                                if cleaned_reason:\n",
    "                                    reason_block = (\n",
    "                                        f'<div class=\"hit-reason\"><span class=\"hit-reason-label\">Model rationale:</span>'\n",
    "                                        f\"{html.escape(cleaned_reason)}</div>\"\n",
    "                                    )\n",
    "                            container.markdown(\n",
    "                                f\"\"\"\n",
    "                                <div class=\"hit-card\">\n",
    "                                    <div class=\"hit-meta\">{meta_line}</div>\n",
    "                                    {reason_block}\n",
    "                                    <div class=\"hit-snippet\">{snippet_html}</div>\n",
    "                                </div>\n",
    "                                \"\"\",\n",
    "                                unsafe_allow_html=True,\n",
    "                            )\n",
    "                        msg = {\"role\": \"assistant\", \"content\": \"**Closest pre-approved answers**\", \"hits\": hits_to_show}\n",
    "                        answer_summary = msg.get(\"content\", \"\")\n",
    "                        if summary_lines:\n",
    "                            answer_summary = f\"{answer_summary}\\n\" + \"\\n\".join(summary_lines)\n",
    "                        answer_summary = answer_summary.strip()\n",
    "                        new_index = len(st.session_state.chat_messages)\n",
    "                        render_chat_feedback_form(\n",
    "                            message_index=new_index,\n",
    "                            question=prompt,\n",
    "                            answer=answer_summary,\n",
    "                            message_payload=msg,\n",
    "                        )\n",
    "                    else:\n",
    "                        empty_message = \"No relevant answers found in the approved library.\"\n",
    "                        container.info(empty_message)\n",
    "                        msg = {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": \"Closest pre-approved answers\",\n",
    "                            \"hits\": [],\n",
    "                            \"empty_message\": empty_message,\n",
    "                        }\n",
    "                        new_index = len(st.session_state.chat_messages)\n",
    "                        render_chat_feedback_form(\n",
    "                            message_index=new_index,\n",
    "                            question=prompt,\n",
    "                            answer=empty_message,\n",
    "                            message_payload=msg,\n",
    "                        )\n",
    "            else:\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    status_placeholder = st.empty()\n",
    "                    message_placeholder = st.empty()\n",
    "\n",
    "                    def _format_answer_status(raw: str) -> str:\n",
    "                        text = (raw or \"\").strip()\n",
    "                        lower = text.lower()\n",
    "                        if \"context\" in lower or \"history\" in lower:\n",
    "                            return \"Stage 1/4 - Analyzing conversation context...\"\n",
    "                        if \"search\" in lower:\n",
    "                            return \"Stage 2/4 - Searching knowledge base...\"\n",
    "                        if \"candidate\" in lower or \"filter\" in lower:\n",
    "                            return \"Stage 3/4 - Filtering snippets...\"\n",
    "                        if \"generating\" in lower:\n",
    "                            return \"Stage 4/4 - Generating answer...\"\n",
    "                        if any(token in lower for token in (\"complete\", \"finished\", \"done\", \"ready\")):\n",
    "                            return \"Stage 4/4 - Answer ready.\"\n",
    "                        return text or \"Working...\"\n",
    "\n",
    "                    def _set_answer_status(message: str, final: bool = False) -> None:\n",
    "                        display = message or \"Working...\"\n",
    "                        if final:\n",
    "                            status_placeholder.success(display)\n",
    "                        else:\n",
    "                            status_placeholder.markdown(\n",
    "                                f'<span class=\"shimmer\">{display}</span>',\n",
    "                                unsafe_allow_html=True,\n",
    "                            )\n",
    "\n",
    "                    def update_status(msg: str) -> None:\n",
    "                        formatted = _format_answer_status(msg)\n",
    "                        lower = (msg or \"\").lower()\n",
    "                        final = any(token in lower for token in (\"complete\", \"finished\", \"done\", \"ready\"))\n",
    "                        _set_answer_status(formatted, final=final)\n",
    "\n",
    "                    _set_answer_status(\"Stage 1/4 - Analyzing conversation context...\")\n",
    "                    intent = _classify_intent(prompt, history)\n",
    "                    follow = _detect_followup(prompt, history) if intent == \"follow_up\" else []\n",
    "                    buf = io.StringIO() if view_mode == \"Developer\" else None\n",
    "                    response_model = llm_model\n",
    "                    restore_client = None\n",
    "                    call_fn = gen_answer if intent == \"follow_up\" else gen\n",
    "                    try:\n",
    "                        if intent == \"follow_up\" and view_mode != \"Developer\":\n",
    "                            response_model = FOLLOWUP_DEFAULT_MODEL\n",
    "                            followup_llm = (\n",
    "                                llm\n",
    "                                if response_model == llm_model\n",
    "                                else (\n",
    "                                    CompletionsClient(model=response_model)\n",
    "                                    if framework == \"aladdin\"\n",
    "                                    else OpenAIClient(model=response_model)\n",
    "                                )\n",
    "                            )\n",
    "                            restore_client = my_module._llm_client\n",
    "                            my_module._llm_client = followup_llm\n",
    "                        if buf:\n",
    "                            with contextlib.redirect_stdout(buf):\n",
    "                                ans = call_fn(prompt, progress=update_status)\n",
    "                        else:\n",
    "                            ans = call_fn(prompt, progress=update_status)\n",
    "                    finally:\n",
    "                        if restore_client is not None:\n",
    "                            my_module._llm_client = restore_client\n",
    "                    _set_answer_status(\"Stage 4/4 - Answer ready.\", final=True)\n",
    "                    debug_text = (\n",
    "                        f\"Intent: {intent}\",\n",
    "                        f\"Follow-up indices: {follow}\",\n",
    "                        f\"{buf.getvalue()}\"\n",
    "                        if buf\n",
    "                        else \"\"\n",
    "                    )\n",
    "                    text = ans.get(\"text\", \"\") if isinstance(ans, dict) else ans\n",
    "                    citations = ans.get(\"citations\", {}) if isinstance(ans, dict) else {}\n",
    "                    message_placeholder.markdown(text)\n",
    "                    label = MODEL_SHORT_NAMES.get(response_model, response_model) if view_mode == \"User\" else response_model\n",
    "                    st.caption(f\"Model: {label}\")\n",
    "                    if view_mode == \"Developer\":\n",
    "                        st.expander(\"Debug info\").markdown(f\"```\\n{debug_text}\\n```\")\n",
    "                    if intent != \"follow_up\":\n",
    "                        my_module.QUESTION_HISTORY.append(prompt)\n",
    "                        my_module.QA_HISTORY.append({\"question\": prompt, \"answer\": text, \"citations\": []})\n",
    "                    payload = {\"role\": \"assistant\", \"content\": text, \"citations\": citations, \"model\": response_model}\n",
    "                    side_idx = sum(\n",
    "                        1 for m in st.session_state.get(\"chat_messages\", []) if m.get(\"role\") == \"assistant\"\n",
    "                    ) + 1\n",
    "                    sidebar.markdown(f\"**Answer {side_idx}**\")\n",
    "                    if citations:\n",
    "                        for lbl, cite in citations.items():\n",
    "                            source_name = (cite.get('source_file') or 'Unknown').strip() or 'Unknown'\n",
    "                            with sidebar.expander(f\"[{lbl}] {source_name}\"):\n",
    "                                section_value = (cite.get('section') or '').strip()\n",
    "                                if section_value:\n",
    "                                    st.markdown(f\"**Section:** {section_value}**\")\n",
    "                                snippet_text = (cite.get('text') or '').strip()\n",
    "                                st.markdown(snippet_text if snippet_text else \"_Snippet not available._\")\n",
    "                    else:\n",
    "                        sidebar.caption(\"No source details returned.\")\n",
    "                    new_index = len(st.session_state.chat_messages)\n",
    "                    render_chat_feedback_form(\n",
    "                        message_index=new_index,\n",
    "                        question=prompt,\n",
    "                        answer=text,\n",
    "                        message_payload=payload,\n",
    "                    )\n",
    "                msg = payload\n",
    "                if view_mode == \"Developer\":\n",
    "                    msg[\"debug\"] = debug_text\n",
    "            st.session_state.chat_messages.append(msg)\n",
    "            history.append(prompt)\n",
    "            st.session_state.question_history = history\n",
    "    else:\n",
    "        run_clicked = st.button(\"Run\")\n",
    "        processing_state = st.session_state.get(\"doc_processing_state\", \"idle\")\n",
    "        processing_result = st.session_state.get(\"doc_processing_result\")\n",
    "        processing_error = st.session_state.get(\"doc_processing_error\")\n",
    "        st.session_state.setdefault(\"doc_downloads\", {})\n",
    "        downloads_container = st.container()\n",
    "        if processing_state == \"started\" and not run_clicked:\n",
    "            st.info(\"Document processing is in progress. Please wait while the run completes.\")\n",
    "        if processing_state == \"error\" and processing_error and not run_clicked:\n",
    "            st.error(f\"Document processing failed: {processing_error}\")\n",
    "            st.session_state.doc_processing_state = \"idle\"\n",
    "            st.session_state.doc_processing_error = None\n",
    "            st.session_state.doc_processing_result = None\n",
    "        if processing_state == \"finished\" and processing_result is not None and not run_clicked:\n",
    "            st.success(\"Document processing completed. You can download the results below or start another run.\")\n",
    "        if run_clicked and processing_state == \"started\":\n",
    "            st.warning(\"A document run is already in progress. Please wait for it to finish.\")\n",
    "            st.stop()\n",
    "        if run_clicked and uploaded is not None and fund:\n",
    "            st.session_state.doc_processing_state = \"started\"\n",
    "            st.session_state.doc_processing_result = None\n",
    "            st.session_state.doc_processing_error = None\n",
    "            st.session_state.doc_processing_started_at = datetime.utcnow().isoformat()\n",
    "            st.session_state.doc_feedback_submitted = False\n",
    "            st.session_state.latest_doc_run = None\n",
    "            _reset_doc_downloads()\n",
    "            run_context = None\n",
    "            phase_placeholder = st.empty()\n",
    "            sub_placeholder = st.empty()\n",
    "            dev_placeholder = st.empty()\n",
    "            dev_logs = []\n",
    "            state = {\"step\": 0, \"phase\": None}\n",
    "            suffix = Path(uploaded.name).suffix.lower()\n",
    "            base_steps = 1\n",
    "            if suffix in (\".xlsx\", \".xls\"):\n",
    "                branch_steps = 4\n",
    "            elif suffix == \".docx\" and not docx_as_text:\n",
    "                branch_steps = 3\n",
    "            else:\n",
    "                branch_steps = 3\n",
    "            total_steps = base_steps + branch_steps + 1\n",
    "            step_bar = st.progress(0)\n",
    "            def log_step(dev_msg, user_msg=None):\n",
    "                if user_msg and user_msg != state[\"phase\"]:\n",
    "                    state[\"phase\"] = user_msg\n",
    "                    phase_placeholder.markdown(f\"**{state['phase']}**\")\n",
    "                    sub_placeholder.empty()\n",
    "                sub_placeholder.markdown(dev_msg)\n",
    "                if view_mode == \"Developer\":\n",
    "                    dev_logs.append(f\"{state['phase']}: {dev_msg}\")\n",
    "                    dev_placeholder.markdown(\"\\n\".join(f\"{i+1}. {m}\" for i, m in enumerate(dev_logs)))\n",
    "                state[\"step\"] += 1\n",
    "                step_bar.progress(state[\"step\"] / total_steps, text=state[\"phase\"])\n",
    "            try:\n",
    "                log_step(\"Saving uploaded file\", \"Preparing document...\")\n",
    "                input_path = save_uploaded_file(uploaded)\n",
    "                extra_docs = [save_uploaded_file(f) for f in extra_uploads] if extra_uploads else None\n",
    "                llm = CompletionsClient(model=llm_model) if framework == \"aladdin\" else OpenAIClient(model=llm_model)\n",
    "                if suffix in (\".xlsx\", \".xls\"):\n",
    "                    log_step(\"Collecting non-empty cells\", \"Reading workbook...\")\n",
    "                    _ = collect_non_empty_cells(input_path)\n",
    "                    log_step(\"Inferring sheet schema\", \"Analyzing workbook structure...\")\n",
    "                    schema = ask_sheet_schema(input_path)\n",
    "                    log_step(\"Building answer generator\", \"Preparing answer generator...\")\n",
    "                    gen = build_generator(\n",
    "                        search_mode,\n",
    "                        fund,\n",
    "                        int(k_max_hits),\n",
    "                        length_opt,\n",
    "                        int(approx_words) if approx_words else None,\n",
    "                        float(min_confidence),\n",
    "                        include_citations,\n",
    "                        llm,\n",
    "                        extra_docs,\n",
    "                    )\n",
    "                    log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                    total_qs = len(schema)\n",
    "                    if total_qs == 0:\n",
    "                        st.info(\"No questions detected in this workbook.\")\n",
    "                        answers = []\n",
    "                    else:\n",
    "                        answers = [None] * total_qs\n",
    "                        worker_limit = _resolve_concurrency(None)\n",
    "                        worker_limit = max(1, min(worker_limit, total_qs))\n",
    "                        progress_container = st.container()\n",
    "                        qa_box = st.container() if show_live else None\n",
    "                        placeholders = [None] * total_qs\n",
    "                        progress = progress_container.progress(0.0)\n",
    "                        futures = {}\n",
    "                        with ThreadPoolExecutor(max_workers=worker_limit) as pool:\n",
    "                            for idx, entry in enumerate(schema):\n",
    "                                question = (entry.get(\"question_text\") or \"\").strip()\n",
    "                                if show_live and qa_box is not None and placeholders[idx] is None:\n",
    "                                    placeholders[idx] = _create_live_placeholder(qa_box, idx, question)\n",
    "                                futures[pool.submit(gen, question)] = (idx, question)\n",
    "                            completed = 0\n",
    "                            for fut in as_completed(futures):\n",
    "                                idx, question = futures[fut]\n",
    "                                ans = fut.result()\n",
    "                                answers[idx] = ans\n",
    "                                completed += 1\n",
    "                                progress.progress(completed / total_qs, text=f\"{completed}/{total_qs}\")\n",
    "                                if show_live and placeholders[idx] is not None:\n",
    "                                    _render_live_answer(placeholders[idx], ans, [], include_citations)\n",
    "                    \n",
    "                    out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\")\n",
    "                    write_excel_answers(\n",
    "                        schema,\n",
    "                        answers,\n",
    "                        input_path,\n",
    "                        out_tmp.name,\n",
    "                        include_comments=include_citations,\n",
    "                    )\n",
    "                    out_tmp.close()\n",
    "                    answered_workbook_name = Path(uploaded.name).stem + \"_answered.xlsx\"\n",
    "                    with open(out_tmp.name, \"rb\") as f:\n",
    "                        workbook_bytes = f.read()\n",
    "                    _store_doc_download(\n",
    "                        \"excel_answer\",\n",
    "                        label=\"Download answered workbook\",\n",
    "                        data=workbook_bytes,\n",
    "                        file_name=answered_workbook_name,\n",
    "                        mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n",
    "                        order=10,\n",
    "                    )\n",
    "                    if include_citations:\n",
    "                        base, _ = os.path.splitext(out_tmp.name)\n",
    "                        comments_path = base + \"_comments.docx\"\n",
    "                        if os.path.exists(comments_path):\n",
    "                            with open(comments_path, \"rb\") as f:\n",
    "                                comments_bytes = f.read()\n",
    "                            _store_doc_download(\n",
    "                                \"excel_comments\",\n",
    "                                label=\"Download comments DOCX\",\n",
    "                                data=comments_bytes,\n",
    "                                file_name=Path(uploaded.name).stem + \"_comments.docx\",\n",
    "                                mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
    "                                order=20,\n",
    "                            )\n",
    "                    _render_doc_downloads(downloads_container)\n",
    "                    qa_pairs = []\n",
    "                    for entry, ans in zip(schema, answers):\n",
    "                        question_text = (entry.get(\"question_text\") or \"\").strip()\n",
    "                        answer_text = ans.get(\"text\", \"\") if isinstance(ans, dict) else ans\n",
    "                        qa_pairs.append({\"question\": question_text, \"answer\": answer_text})\n",
    "                    run_context = {\n",
    "                        \"mode\": \"excel\",\n",
    "                        \"uploaded_name\": uploaded.name,\n",
    "                        \"fund\": fund,\n",
    "                        \"search_mode\": search_mode,\n",
    "                        \"include_citations\": include_citations,\n",
    "                        \"length\": length_opt,\n",
    "                        \"approx_words\": approx_words,\n",
    "                        \"extra_documents\": [f.name for f in extra_uploads] if extra_uploads else [],\n",
    "                        \"qa_pairs\": qa_pairs,\n",
    "                        \"schema\": schema,\n",
    "                        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    }\n",
    "                elif suffix == \".docx\" and not docx_as_text:\n",
    "                    log_step(\"Extracting slots from DOCX\", \"Analyzing document...\")\n",
    "                    slots_payload = extract_slots_from_docx(input_path)\n",
    "                    slots_tmp = tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False, suffix=\".json\")\n",
    "                    json.dump(slots_payload, slots_tmp)\n",
    "                    slots_tmp.flush()\n",
    "                    log_step(\"Building answer generator\", \"Preparing answer generator...\")\n",
    "                    gen = build_generator(\n",
    "                        search_mode,\n",
    "                        fund,\n",
    "                        int(k_max_hits),\n",
    "                        length_opt,\n",
    "                        int(approx_words) if approx_words else None,\n",
    "                        float(min_confidence),\n",
    "                        include_citations,\n",
    "                        llm,\n",
    "                        extra_docs,\n",
    "                    )\n",
    "                    log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                    slot_list = slots_payload.get('slots', [])\n",
    "                    skipped_entries = slots_payload.get('skipped_slots', [])\n",
    "                    heuristic_entries = slots_payload.get('heuristic_skips', [])\n",
    "                    combined_skips = []\n",
    "                    if skipped_entries:\n",
    "                        for entry in skipped_entries:\n",
    "                            combined_skips.append({\n",
    "                                'question_text': (entry.get('question_text') or '').strip() or '[blank question text]',\n",
    "                                'reason_key': entry.get('reason') or 'unspecified',\n",
    "                                'source': 'slot_filter',\n",
    "                            })\n",
    "                    if heuristic_entries:\n",
    "                        for entry in heuristic_entries:\n",
    "                            combined_skips.append({\n",
    "                                'question_text': (entry.get('question_text') or '').strip() or '[blank question text]',\n",
    "                                'reason': entry.get('reason', 'unspecified'),\n",
    "                                'source': 'heuristic',\n",
    "                            })\n",
    "                    if combined_skips:\n",
    "                        reason_labels = {\n",
    "                            'table_reference': 'references a table',\n",
    "                            'blank_question_text': 'blank question text',\n",
    "                            'heuristic_veto': 'failed question heuristics',\n",
    "                            'llm_veto': 'LLM candidate rejected by heuristics',\n",
    "                        }\n",
    "                        st.warning(f\"Skipped {len(combined_skips)} question(s) that cannot be answered automatically.\")\n",
    "                        with st.expander('View skipped questions', expanded=False):\n",
    "                            for entry in combined_skips:\n",
    "                                if entry['source'] == 'slot_filter':\n",
    "                                    reason_label = reason_labels.get(entry['reason_key'], entry['reason_key'] or 'unspecified')\n",
    "                                else:\n",
    "                                    reason_label = entry.get('reason', 'unspecified')\n",
    "                                st.markdown(f\"- **{entry['question_text']}** \u2014 {reason_label}\")\n",
    "\n",
    "                    num_q = len(slot_list)\n",
    "                    answers_dict = {}\n",
    "                    if num_q == 0:\n",
    "                        st.info(\"No questions detected in this document.\")\n",
    "                    else:\n",
    "                        max_workers = _resolve_concurrency(None)\n",
    "                        max_workers = max(1, min(max_workers, num_q))\n",
    "\n",
    "                        def _make_client():\n",
    "                            return CompletionsClient(model=llm_model) if framework == \"aladdin\" else OpenAIClient(model=llm_model)\n",
    "\n",
    "                        clients = [_make_client() for _ in range(max_workers)]\n",
    "                        gens = [\n",
    "                            build_generator(\n",
    "                                search_mode,\n",
    "                                fund,\n",
    "                                int(k_max_hits),\n",
    "                                length_opt,\n",
    "                                int(approx_words) if approx_words else None,\n",
    "                                float(min_confidence),\n",
    "                                include_citations,\n",
    "                                client,\n",
    "                                extra_docs,\n",
    "                            )\n",
    "                            for client in clients\n",
    "                        ]\n",
    "\n",
    "                        placeholders = [None] * num_q\n",
    "                        progress_container = st.container()\n",
    "                        qa_box = st.container() if show_live else None\n",
    "                        if show_live and qa_box is not None:\n",
    "                            for idx, slot in enumerate(slot_list):\n",
    "                                q_text = (slot.get(\"question_text\") or \"\").strip()\n",
    "                                placeholders[idx] = _create_live_placeholder(qa_box, idx, q_text)\n",
    "\n",
    "                        progress = progress_container.progress(0.0)\n",
    "\n",
    "                        def _safe_generate(worker_id: int, i: int, slot: dict):\n",
    "                            gen_fn = gens[worker_id]\n",
    "                            q_text = (slot.get(\"question_text\") or \"\").strip()\n",
    "                            attempts = int(os.getenv(\"RFP_QA_RETRIES\", \"3\"))\n",
    "                            base_sleep = float(os.getenv(\"RFP_QA_BACKOFF\", \"1.5\"))\n",
    "                            for attempt in range(attempts):\n",
    "                                try:\n",
    "                                    result = gen_fn(q_text)\n",
    "                                    return (i, slot.get(\"id\", f\"slot_{i}\"), q_text, result, None)\n",
    "                                except Exception as exc:\n",
    "                                    if attempt + 1 == attempts:\n",
    "                                        return (i, slot.get(\"id\", f\"slot_{i}\"), q_text, None, exc)\n",
    "                                    time.sleep((base_sleep ** attempt) + random.random() * 0.35)\n",
    "\n",
    "                        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "                            futures = []\n",
    "                            for i, slot in enumerate(slot_list, 1):\n",
    "                                worker_id = (i - 1) % max_workers\n",
    "                                futures.append(ex.submit(_safe_generate, worker_id, i, slot))\n",
    "\n",
    "                            completed = 0\n",
    "                            for fut in as_completed(futures):\n",
    "                                i, slot_id, q_text, ans, err = fut.result()\n",
    "                                completed += 1\n",
    "                                progress.progress(completed / num_q, text=f\"{completed}/{num_q}\")\n",
    "\n",
    "                                if err is not None:\n",
    "                                    err_msg = f\"[temporary error] Could not generate answer. ({err})\"\n",
    "                                    answers_dict[slot_id] = {\"text\": err_msg} if include_citations else err_msg\n",
    "                                    if show_live and placeholders[i - 1] is not None:\n",
    "                                        err_payload = {\"text\": err_msg} if include_citations else err_msg\n",
    "                                        _render_live_answer(placeholders[i - 1], err_payload, [], include_citations)\n",
    "                                    continue\n",
    "\n",
    "                                slot_meta = slot_list[i - 1] if 0 <= (i - 1) < len(slot_list) else {}\n",
    "                                processed_answer = ans\n",
    "                                if slot_meta and _is_table_slot(slot_meta):\n",
    "                                    processed_answer = _sanitize_table_answer(ans)\n",
    "                                answers_dict[slot_id] = processed_answer\n",
    "                                if show_live and placeholders[i - 1] is not None:\n",
    "                                    comments = []\n",
    "                                    if include_citations and isinstance(processed_answer, dict):\n",
    "                                        raw_comments = processed_answer.get('citations') or processed_answer.get('comments') or []\n",
    "                                        if isinstance(raw_comments, list):\n",
    "                                            comments = raw_comments\n",
    "                                        elif raw_comments:\n",
    "                                            comments = [raw_comments]\n",
    "                                    _render_live_answer(\n",
    "                                        placeholders[i - 1],\n",
    "                                        processed_answer,\n",
    "                                        comments,\n",
    "                                        include_citations and isinstance(processed_answer, dict),\n",
    "                                    )\n",
    "\n",
    "                    answers_tmp = tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False, suffix=\".json\")\n",
    "                    json.dump({'by_id': answers_dict}, answers_tmp)\n",
    "                    answers_tmp.flush()\n",
    "                    answers_tmp.close()\n",
    "                    out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".docx\")\n",
    "                    apply_answers_to_docx(\n",
    "                        docx_path=input_path,\n",
    "                        slots_json_path=slots_tmp.name,\n",
    "                        answers_json_path=answers_tmp.name,\n",
    "                        out_path=out_tmp.name,\n",
    "                        mode=docx_write_mode,\n",
    "                        generator=None,\n",
    "                        gen_name=\"streamlit_app:rag_gen\",\n",
    "                    )\n",
    "                    out_tmp.close()\n",
    "                    answered_doc_name = Path(uploaded.name).stem + \"_answered.docx\"\n",
    "                    with open(out_tmp.name, \"rb\") as f:\n",
    "                        answered_bytes = f.read()\n",
    "                    _store_doc_download(\n",
    "                        \"docx_answer\",\n",
    "                        label=\"Download answered DOCX\",\n",
    "                        data=answered_bytes,\n",
    "                        file_name=answered_doc_name,\n",
    "                        mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
    "                        order=10,\n",
    "                    )\n",
    "                    _render_doc_downloads(downloads_container)\n",
    "                    qa_pairs = []\n",
    "                    slot_list = slots_payload.get('slots', [])\n",
    "                    for slot in slot_list:\n",
    "                        question_text = (slot.get(\"question_text\") or \"\").strip()\n",
    "                        answer_obj = answers_dict.get(slot.get('id'))\n",
    "                        answer_text = \"\"\n",
    "                        if isinstance(answer_obj, dict):\n",
    "                            answer_text = answer_obj.get(\"text\", \"\")\n",
    "                        elif isinstance(answer_obj, str):\n",
    "                            answer_text = answer_obj\n",
    "                        qa_pairs.append({\"question\": question_text, \"answer\": answer_text})\n",
    "                    run_context = {\n",
    "                        \"mode\": \"docx_slots\",\n",
    "                        \"uploaded_name\": uploaded.name,\n",
    "                        \"fund\": fund,\n",
    "                        \"search_mode\": search_mode,\n",
    "                        \"include_citations\": include_citations,\n",
    "                        \"docx_write_mode\": docx_write_mode,\n",
    "                        \"extra_documents\": [f.name for f in extra_uploads] if extra_uploads else [],\n",
    "                        \"qa_pairs\": qa_pairs,\n",
    "                        \"slots\": slots_payload,\n",
    "                        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    }\n",
    "                else:\n",
    "                    log_step(\"Loading input text\", \"Reading document...\")\n",
    "                    raw = load_input_text(input_path)\n",
    "                    log_step(\"Extracting questions\", \"Finding questions...\")\n",
    "                    questions = extract_questions(raw, llm)\n",
    "                    log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                    total_qs = len(questions)\n",
    "                    if total_qs == 0:\n",
    "                        st.info(\"No questions could be extracted from the document.\")\n",
    "                        answers = []\n",
    "                        comments = []\n",
    "                    else:\n",
    "                        answers = [None] * total_qs\n",
    "                        comments = [None] * total_qs\n",
    "                        worker_limit = _resolve_concurrency(None)\n",
    "                        worker_limit = max(1, min(worker_limit, total_qs))\n",
    "                        progress_container = st.container()\n",
    "                        qa_box = st.container() if show_live else None\n",
    "                        placeholders = [None] * total_qs\n",
    "                        progress = progress_container.progress(0.0)\n",
    "\n",
    "                        def _generate_text_answer(question_text: str):\n",
    "                            return answer_question(\n",
    "                                question_text,\n",
    "                                search_mode,\n",
    "                                fund,\n",
    "                                int(k_max_hits),\n",
    "                                length_opt,\n",
    "                                int(approx_words) if approx_words else None,\n",
    "                                float(min_confidence),\n",
    "                                llm,\n",
    "                            )\n",
    "\n",
    "                        futures = {}\n",
    "                        with ThreadPoolExecutor(max_workers=worker_limit) as pool:\n",
    "                            for idx, q in enumerate(questions):\n",
    "                                if show_live and qa_box is not None and placeholders[idx] is None:\n",
    "                                    placeholders[idx] = _create_live_placeholder(qa_box, idx, q)\n",
    "                                futures[pool.submit(_generate_text_answer, q)] = idx\n",
    "                            completed = 0\n",
    "                            for fut in as_completed(futures):\n",
    "                                idx = futures[fut]\n",
    "                                ans, cmts = fut.result()\n",
    "                                if not include_citations:\n",
    "                                    ans = re.sub(r\"\\\\[\\\\d+\\\\]\", \"\", ans)\n",
    "                                    cmts = []\n",
    "                                answers[idx] = ans\n",
    "                                comments[idx] = cmts\n",
    "                                completed += 1\n",
    "                                progress.progress(completed / total_qs, text=f\"{completed}/{total_qs}\")\n",
    "                                if show_live and placeholders[idx] is not None:\n",
    "                                    _render_live_answer(placeholders[idx], ans, cmts, include_citations)\n",
    "                    \n",
    "                    qa_doc = build_docx(\n",
    "                        questions,\n",
    "                        answers,\n",
    "                        comments,\n",
    "                        include_comments=include_citations,\n",
    "                    )\n",
    "                    answered_doc_name = Path(uploaded.name).stem + \"_answered.docx\"\n",
    "                    _store_doc_download(\n",
    "                        \"document_summary\",\n",
    "                        label=\"Download Q/A report\",\n",
    "                        data=qa_doc,\n",
    "                        file_name=answered_doc_name,\n",
    "                        mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
    "                        order=10,\n",
    "                    )\n",
    "                    _render_doc_downloads(downloads_container)\n",
    "                    qa_pairs = []\n",
    "                    for question, answer_text in zip(questions, answers):\n",
    "                        rendered_answer = answer_text.get(\"text\", \"\") if isinstance(answer_text, dict) else answer_text\n",
    "                        qa_pairs.append({\"question\": question, \"answer\": rendered_answer})\n",
    "                    run_context = {\n",
    "                        \"mode\": \"document_summary\",\n",
    "                        \"uploaded_name\": uploaded.name,\n",
    "                        \"fund\": fund,\n",
    "                        \"search_mode\": search_mode,\n",
    "                        \"include_citations\": include_citations,\n",
    "                        \"length\": length_opt,\n",
    "                        \"approx_words\": approx_words,\n",
    "                        \"extra_documents\": [f.name for f in extra_uploads] if extra_uploads else [],\n",
    "                        \"qa_pairs\": qa_pairs,\n",
    "                        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    }\n",
    "                step_bar.progress(1.0, text=\"Done\")\n",
    "                st.session_state.doc_processing_state = \"finished\"\n",
    "                st.session_state.doc_processing_finished_at = datetime.utcnow().isoformat()\n",
    "                st.session_state.doc_processing_result = run_context\n",
    "                if run_context:\n",
    "                    st.session_state.latest_doc_run = run_context\n",
    "                    render_document_feedback_section(run_context)\n",
    "            except Exception as exc:\n",
    "                st.session_state.doc_processing_state = \"error\"\n",
    "                st.session_state.doc_processing_error = str(exc)\n",
    "                _reset_doc_downloads()\n",
    "                st.error(\"Something went wrong while processing the document. Please try again.\")\n",
    "                st.exception(exc)\n",
    "                st.stop()\n",
    "        elif run_clicked and not fund:\n",
    "            st.warning(\"Please select a fund or strategy before running.\")\n",
    "        elif run_clicked:\n",
    "            st.warning(\"Please upload a document before running.\")\n",
    "        elif st.session_state.get(\"latest_doc_run\"):\n",
    "            render_document_feedback_section(st.session_state.get(\"latest_doc_run\"))\n",
    "\n",
    "        if processing_state == \"finished\" and not run_clicked:\n",
    "            _render_doc_downloads(downloads_container)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
