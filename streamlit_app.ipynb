{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import streamlit as st\n",
    "\n",
    "@st.cache_resource\n",
    "def install_packages(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "packages = [\n",
    "    \"certifi\",\n",
    "    \"charset-normalizer\",\n",
    "    \"faiss-cpu\",\n",
    "    \"idna\",\n",
    "    \"numpy\",\n",
    "    \"packaging\",\n",
    "    \"python-dotenv\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"pyarrow\",\n",
    "    \"PyPDF2\",\n",
    "    \"python-docx\",\n",
    "    \"spacy\",\n",
    "]\n",
    "\n",
    "if \"setup_complete\" not in st.session_state:\n",
    "    progress_placeholder = st.empty()\n",
    "    progress_bar = progress_placeholder.progress(0, text=\"Setting up the application...\")\n",
    "    total = len(packages)\n",
    "    for i, package in enumerate(packages, start=1):\n",
    "        progress_bar.progress(int((i - 1) / total * 100), text=f\"Setting up step {i} of {total}...\")\n",
    "        try:\n",
    "            install_packages(package)\n",
    "        except subprocess.CalledProcessError:\n",
    "            progress_placeholder.empty()\n",
    "            st.error(\"Something went wrong while setting things up. Please try again or contact support.\")\n",
    "            break\n",
    "        progress_bar.progress(int(i / total * 100), text=f\"Finished step {i} of {total}\")\n",
    "    else:\n",
    "        progress_placeholder.empty()\n",
    "        st.session_state.setup_complete = True\n",
    "        st.toast(\"You're all set! Choose 'Upload document' to load an RFP or 'Ask a question' to chat. Use the sidebar to switch interface modes and provide any required API keys.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e011114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Callable\n",
    "\n",
    "from cli_app import (\n",
    "    load_input_text,\n",
    "    extract_questions,\n",
    "    build_docx,\n",
    ")\n",
    "from qa_core import answer_question\n",
    "from answer_composer import CompletionsClient, get_openai_completion\n",
    "from input_file_reader.interpreter_sheet import collect_non_empty_cells\n",
    "from rfp_xlsx_slot_finder import ask_sheet_schema\n",
    "from rfp_xlsx_apply_answers import write_excel_answers\n",
    "from rfp_docx_slot_finder import extract_slots_from_docx\n",
    "from rfp_docx_apply_answers import apply_answers_to_docx\n",
    "import my_module\n",
    "from my_module import _classify_intent, _detect_followup, gen_answer\n",
    "MODEL_DESCRIPTIONS = {\n",
    "    \"gpt-4.1-nano-2025-04-14_research\": \"Lighter, faster model\",\n",
    "    \"o3-2025-04-16_research\": \"Slower, reasoning model\",\n",
    "}\n",
    "MODEL_SHORT_NAMES = {\n",
    "    \"gpt-4.1-nano-2025-04-14_research\": \"4.1\",\n",
    "    \"o3-2025-04-16_research\": \"o3\",\n",
    "}\n",
    "MODEL_OPTIONS = list(MODEL_DESCRIPTIONS.keys())\n",
    "\n",
    "\n",
    "def load_fund_tags() -> List[str]:\n",
    "    path = Path('~/derivs-tool/rfp-ai-tool/structured_extraction/embedding_data.json').expanduser()\n",
    "    try:\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception:\n",
    "        return []\n",
    "    tags = {t for item in data for t in item.get('metadata', {}).get('tags', [])}\n",
    "    return sorted(tags)\n",
    "\n",
    "\n",
    "class OpenAIClient:\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "\n",
    "    def get_completion(self, prompt: str, json_output: bool = False):\n",
    "        return get_openai_completion(prompt, self.model, json_output=json_output)\n",
    "\n",
    "\n",
    "def save_uploaded_file(uploaded_file) -> str:\n",
    "    suffix = Path(uploaded_file.name).suffix\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n",
    "    tmp.write(uploaded_file.read())\n",
    "    tmp.flush()\n",
    "    return tmp.name\n",
    "\n",
    "\n",
    "def build_generator(\n",
    "    search_mode: str,\n",
    "    fund: Optional[str],\n",
    "    k: int,\n",
    "    length: Optional[str],\n",
    "    approx_words: Optional[int],\n",
    "    min_confidence: float,\n",
    "    include_citations: bool,\n",
    "    llm,\n",
    "    extra_docs: Optional[List[str]] = None,\n",
    "):\n",
    "    def gen(question: str, progress: Optional[Callable[[str], None]] = None):\n",
    "        ans, cmts = answer_question(\n",
    "            question,\n",
    "            search_mode,\n",
    "            fund,\n",
    "            k,\n",
    "            length,\n",
    "            approx_words,\n",
    "            min_confidence,\n",
    "            llm,\n",
    "            extra_docs=extra_docs,\n",
    "            progress=progress,\n",
    "        )\n",
    "        if not include_citations:\n",
    "            ans = re.sub(r\"\\[\\d+\\]\", \"\", ans)\n",
    "            return ans\n",
    "        citations = {\n",
    "            lbl: {\"text\": snippet, \"source_file\": src}\n",
    "            for lbl, src, snippet, score, date in cmts\n",
    "        }\n",
    "        return {\"text\": ans, \"citations\": citations}\n",
    "\n",
    "    return gen\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.title(\"RFP Responder\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        div.block-container{\n",
    "            max-width: 750px;\n",
    "            padding-top: 2rem;\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage\"]{\n",
    "            border-radius: 0.5rem;\n",
    "            padding: 1rem;\n",
    "            margin-bottom: 1rem;\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage-user\"]{\n",
    "            background-color: #DCF8C6;\n",
    "        }\n",
    "        div[data-testid=\"stChatMessage-assistant\"]{\n",
    "            background-color: #FFFFFF;\n",
    "        }\n",
    "        div[data-testid=\"stChatInput\"] textarea{\n",
    "            border-radius: 0.5rem;\n",
    "            padding: 0.75rem;\n",
    "        }\n",
    "        @keyframes shimmer{\n",
    "            0%{background-position:-1000px 0;}\n",
    "            100%{background-position:1000px 0;}\n",
    "        }\n",
    "        .shimmer{\n",
    "            background:linear-gradient(90deg,#d0d0d0 0%,#b0b0b0 50%,#d0d0d0 100%);\n",
    "            background-size:1000px 100%;\n",
    "            animation:shimmer 2s infinite linear;\n",
    "            -webkit-background-clip:text;\n",
    "            -webkit-text-fill-color:transparent;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "    view_mode = st.sidebar.radio(\"Interface mode\", [\"User\", \"Developer\"], index=0)\n",
    "    input_mode = st.radio(\"How would you like to proceed?\", [\"Upload document\", \"Ask a question\"], index=1, horizontal=True)\n",
    "    llm_model = MODEL_OPTIONS[0]\n",
    "\n",
    "    framework_env = os.getenv(\"ANSWER_FRAMEWORK\")\n",
    "    if framework_env:\n",
    "        if view_mode == \"Developer\":\n",
    "            st.info(f\"Using framework from ANSWER_FRAMEWORK: {framework_env}\")\n",
    "        framework = framework_env\n",
    "    else:\n",
    "        framework = st.selectbox(\"Framework\", [\"aladdin\", \"openai\"], index=0, help=\"Choose backend for language model.\")\n",
    "\n",
    "    if framework == \"aladdin\":\n",
    "        for key, label in [\n",
    "            (\"aladdin_studio_api_key\", \"Aladdin Studio API key\"),\n",
    "            (\"defaultWebServer\", \"Default Web Server\"),\n",
    "            (\"aladdin_user\", \"Aladdin user\"),\n",
    "            (\"aladdin_passwd\", \"Aladdin password\"),\n",
    "        ]:\n",
    "            if os.getenv(key):\n",
    "                if view_mode == \"Developer\":\n",
    "                    st.info(f\"{key} loaded from environment\")\n",
    "            else:\n",
    "                val = st.text_input(label, type=\"password\" if \"passwd\" in key or \"api_key\" in key else \"default\")\n",
    "                if val:\n",
    "                    os.environ[key] = val\n",
    "    else:\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            if view_mode == \"Developer\":\n",
    "                st.info(\"OPENAI_API_KEY loaded from environment\")\n",
    "        else:\n",
    "            api_key = st.text_input(\"OpenAI API key\", type=\"password\", help=\"API key for OpenAI.\")\n",
    "            if api_key:\n",
    "                os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "    if input_mode == \"Upload document\":\n",
    "        uploaded = st.file_uploader(\n",
    "            \"Upload document\",\n",
    "            type=[\"pdf\", \"docx\", \"txt\", \"xlsx\"],\n",
    "            help=\"Upload the RFP or question file.\",\n",
    "        )\n",
    "    else:\n",
    "        uploaded = None\n",
    "\n",
    "    if view_mode == \"Developer\":\n",
    "        st.info(\"Search mode fixed to 'both'\")\n",
    "        search_mode = \"both\"\n",
    "        fund = st.selectbox(\n",
    "            \"Fund\", [\"\"] + load_fund_tags(), index=0,\n",
    "            help=\"Filter answers for a specific fund or strategy.\",\n",
    "        )\n",
    "        llm_model = st.selectbox(\n",
    "            \"LLM model\",\n",
    "            MODEL_OPTIONS,\n",
    "            index=0,\n",
    "            format_func=lambda m: f\"{m} - {MODEL_DESCRIPTIONS[m]}\",\n",
    "            help=\"Model name for generating answers.\",\n",
    "        )\n",
    "        k_max_hits = st.number_input(\"Hits per question\", value=20, help=\"Maximum documents retrieved per question.\")\n",
    "        min_confidence = st.number_input(\"Min confidence\", value=0.0, help=\"Minimum score for retrieved documents.\")\n",
    "        docx_as_text = st.checkbox(\"Treat DOCX as text\", value=False)\n",
    "        docx_write_mode = st.selectbox(\"DOCX write mode\", [\"fill\", \"replace\", \"append\"], index=0)\n",
    "        extra_uploads = st.file_uploader(\n",
    "            \"Additional documents\", type=[\"pdf\", \"docx\", \"txt\"], accept_multiple_files=True\n",
    "        )\n",
    "    else:\n",
    "        st.markdown(\"### Settings\")\n",
    "        search_mode = \"both\"\n",
    "        fund = st.selectbox(\n",
    "            \"Fund\", [\"\"] + load_fund_tags(), index=0,\n",
    "            help=\"Select fund or strategy context for better answers.\",\n",
    "        )\n",
    "        k_max_hits = 20\n",
    "        min_confidence = 0.0\n",
    "        docx_as_text = False\n",
    "        docx_write_mode = \"fill\"\n",
    "        extra_uploads = None\n",
    "\n",
    "    with st.expander(\"More options\"):\n",
    "        if view_mode == \"User\":\n",
    "            llm_model = st.selectbox(\n",
    "                \"Model\",\n",
    "                MODEL_OPTIONS,\n",
    "                index=MODEL_OPTIONS.index(llm_model),\n",
    "                format_func=lambda m: f\"{MODEL_SHORT_NAMES[m]} - {MODEL_DESCRIPTIONS[m]}\",\n",
    "                help=\"Choose which model generates answers.\",\n",
    "            )\n",
    "        length_opt = st.selectbox(\"Answer length\", [\"auto\", \"short\", \"medium\", \"long\"], index=3)\n",
    "        approx_words = st.text_input(\"Approx words\", value=\"\", help=\"Approximate words per answer (optional).\")\n",
    "        include_env = os.getenv(\"RFP_INCLUDE_COMMENTS\")\n",
    "        if include_env is not None:\n",
    "            include_citations = include_env != \"0\"\n",
    "            st.info(f\"Using include citations from RFP_INCLUDE_COMMENTS: {include_citations}\")\n",
    "        else:\n",
    "            include_citations = st.checkbox(\"Include citations\", value=True)\n",
    "        show_live = st.checkbox(\"Show questions and answers during processing\", value=True)\n",
    "\n",
    "\n",
    "    if input_mode == \"Ask a question\":\n",
    "        llm = CompletionsClient(model=llm_model) if framework == \"aladdin\" else OpenAIClient(model=llm_model)\n",
    "        gen = build_generator(\n",
    "            search_mode,\n",
    "            fund,\n",
    "            int(k_max_hits),\n",
    "            length_opt,\n",
    "            int(approx_words) if approx_words else None,\n",
    "            float(min_confidence),\n",
    "            include_citations,\n",
    "            llm,\n",
    "        )\n",
    "        my_module._llm_client = llm\n",
    "        if \"chat_messages\" not in st.session_state:\n",
    "            st.session_state.chat_messages = []\n",
    "        if \"question_history\" not in st.session_state:\n",
    "            st.session_state.question_history = []\n",
    "        sidebar = st.sidebar.container()\n",
    "        sidebar.markdown(\"### References\")\n",
    "        answer_idx = 0\n",
    "        for msg in st.session_state.chat_messages:\n",
    "            with st.chat_message(msg[\"role\"]):\n",
    "                st.markdown(msg[\"content\"])\n",
    "                if msg.get(\"model\"):\n",
    "                    name = MODEL_SHORT_NAMES.get(msg[\"model\"], msg[\"model\"]) if view_mode == \"User\" else msg[\"model\"]\n",
    "                    st.caption(f\"Model: {name}\")\n",
    "                if view_mode == \"Developer\" and msg.get(\"debug\"):\n",
    "                    st.expander(\"Debug info\").markdown(f\"```\\n{msg['debug']}\\n```\")\n",
    "                if msg[\"role\"] == \"assistant\":\n",
    "                    answer_idx += 1\n",
    "                    sidebar.markdown(f\"**Answer {answer_idx}**\")\n",
    "                    for lbl, cite in (msg.get(\"citations\") or {}).items():\n",
    "                        with sidebar.expander(str(lbl)):\n",
    "                            st.markdown(f\"**Document:** {cite.get('source_file', 'Unknown')}**\")\n",
    "                            section = cite.get('section')\n",
    "                            if section:\n",
    "                                st.markdown(f\"**Section:** {section}**\")\n",
    "                            st.markdown(cite.get('text', ''))\n",
    "        if prompt := st.chat_input(\"Ask a question\"):\n",
    "            st.chat_message(\"user\").markdown(prompt)\n",
    "            st.session_state.chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                message_placeholder = st.empty()\n",
    "                def update_status(msg: str):\n",
    "                    message_placeholder.markdown(f'<span class=\"shimmer\">{msg}</span>', unsafe_allow_html=True)\n",
    "                update_status(\"Thinking...\")\n",
    "                history = st.session_state.get(\"question_history\", [])\n",
    "                intent = _classify_intent(prompt, history)\n",
    "                follow = _detect_followup(prompt, history) if intent == \"follow_up\" else []\n",
    "                buf = io.StringIO() if view_mode == \"Developer\" else None\n",
    "                if buf:\n",
    "                    with contextlib.redirect_stdout(buf):\n",
    "                        ans = gen_answer(prompt, progress=update_status) if intent == \"follow_up\" else gen(prompt, progress=update_status)\n",
    "                    debug_text = f\"Intent: {intent}\\nFollow-up indices: {follow}\\n\" + buf.getvalue()\n",
    "                else:\n",
    "                    ans = gen_answer(prompt, progress=update_status) if intent == \"follow_up\" else gen(prompt, progress=update_status)\n",
    "                    debug_text = \"\"\n",
    "                text = ans.get(\"text\", \"\") if isinstance(ans, dict) else ans\n",
    "                citations = ans.get(\"citations\", {}) if isinstance(ans, dict) else {}\n",
    "                message_placeholder.markdown(text)\n",
    "                label = MODEL_SHORT_NAMES.get(llm_model, llm_model) if view_mode == \"User\" else llm_model\n",
    "                st.caption(f\"Model: {label}\")\n",
    "                if view_mode == \"Developer\":\n",
    "                    st.expander(\"Debug info\").markdown(f\"```\\n{debug_text}\\n```\")\n",
    "                if intent != \"follow_up\":\n",
    "                    my_module.QUESTION_HISTORY.append(prompt)\n",
    "                    my_module.QA_HISTORY.append({\"question\": prompt, \"answer\": text, \"citations\": []})\n",
    "            msg = {\"role\": \"assistant\", \"content\": text, \"citations\": citations, \"model\": llm_model}\n",
    "            if view_mode == \"Developer\":\n",
    "                msg[\"debug\"] = debug_text\n",
    "            st.session_state.chat_messages.append(msg)\n",
    "            history = st.session_state.get(\"question_history\", [])\n",
    "            history.append(prompt)\n",
    "            st.session_state.question_history = history\n",
    "            st.rerun()\n",
    "    else:\n",
    "        run_clicked = st.button(\"Run\")\n",
    "        if run_clicked and uploaded is not None and fund:\n",
    "            phase_placeholder = st.empty()\n",
    "            sub_placeholder = st.empty()\n",
    "            dev_placeholder = st.empty()\n",
    "            dev_logs = []\n",
    "            state = {\"step\": 0, \"phase\": None}\n",
    "            suffix = Path(uploaded.name).suffix.lower()\n",
    "            base_steps = 1\n",
    "            if suffix in (\".xlsx\", \".xls\"):\n",
    "                branch_steps = 4\n",
    "            elif suffix == \".docx\" and not docx_as_text:\n",
    "                branch_steps = 3\n",
    "            else:\n",
    "                branch_steps = 3\n",
    "            total_steps = base_steps + branch_steps + 1\n",
    "            step_bar = st.progress(0)\n",
    "            def log_step(dev_msg, user_msg=None):\n",
    "                if user_msg and user_msg != state[\"phase\"]:\n",
    "                    state[\"phase\"] = user_msg\n",
    "                    phase_placeholder.markdown(f\"**{state['phase']}**\")\n",
    "                    sub_placeholder.empty()\n",
    "                sub_placeholder.markdown(dev_msg)\n",
    "                if view_mode == \"Developer\":\n",
    "                    dev_logs.append(f\"{state['phase']}: {dev_msg}\")\n",
    "                    dev_placeholder.markdown(\"\\n\".join(f\"{i+1}. {m}\" for i, m in enumerate(dev_logs)))\n",
    "                state[\"step\"] += 1\n",
    "                step_bar.progress(state[\"step\"] / total_steps, text=state[\"phase\"])\n",
    "            log_step(\"Saving uploaded file\", \"Preparing document...\")\n",
    "            input_path = save_uploaded_file(uploaded)\n",
    "            extra_docs = [save_uploaded_file(f) for f in extra_uploads] if extra_uploads else None\n",
    "            llm = CompletionsClient(model=llm_model) if framework == \"aladdin\" else OpenAIClient(model=llm_model)\n",
    "            if suffix in (\".xlsx\", \".xls\"):\n",
    "                log_step(\"Collecting non-empty cells\", \"Reading workbook...\")\n",
    "                cells = collect_non_empty_cells(input_path)\n",
    "                log_step(\"Inferring sheet schema\", \"Analyzing workbook structure...\")\n",
    "                schema = ask_sheet_schema(input_path)\n",
    "                log_step(\"Building answer generator\", \"Preparing answer generator...\")\n",
    "                gen = build_generator(\n",
    "                    search_mode,\n",
    "                    fund,\n",
    "                    int(k_max_hits),\n",
    "                    length_opt,\n",
    "                    int(approx_words) if approx_words else None,\n",
    "                    float(min_confidence),\n",
    "                    include_citations,\n",
    "                    llm,\n",
    "                    extra_docs,\n",
    "                )\n",
    "                log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                answers = []\n",
    "                total_qs = len(schema)\n",
    "                progress = st.progress(0)\n",
    "                qa_box = st.container() if show_live else None\n",
    "                for i, entry in enumerate(schema, 1):\n",
    "                    question = (entry.get(\"question_text\") or \"\").strip()\n",
    "                    if show_live and question:\n",
    "                        qa_box.markdown(f\"**Q{i}:** {question}\")\n",
    "                    ans = gen(question)\n",
    "                    answers.append(ans)\n",
    "                    progress.progress(i / total_qs, text=f\"{i}/{total_qs}\")\n",
    "                    if show_live:\n",
    "                        text = ans.get('text', '') if isinstance(ans, dict) else ans\n",
    "                        qa_box.markdown(f\"**A{i}:** {text}\")\n",
    "                out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\")\n",
    "                write_excel_answers(\n",
    "                    schema,\n",
    "                    answers,\n",
    "                    input_path,\n",
    "                    out_tmp.name,\n",
    "                    include_comments=include_citations,\n",
    "                )\n",
    "                with open(out_tmp.name, \"rb\") as f:\n",
    "                    st.download_button(\n",
    "                        \"Download answered workbook\",\n",
    "                        f,\n",
    "                        file_name=Path(uploaded.name).stem + \"_answered.xlsx\",\n",
    "                    )\n",
    "                if include_citations:\n",
    "                    base, _ = os.path.splitext(out_tmp.name)\n",
    "                    comments_path = base + \"_comments.docx\"\n",
    "                    if os.path.exists(comments_path):\n",
    "                        with open(comments_path, \"rb\") as f:\n",
    "                            st.download_button(\n",
    "                                \"Download comments DOCX\",\n",
    "                                f,\n",
    "                                file_name=Path(uploaded.name).stem + \"_comments.docx\",\n",
    "                            )\n",
    "            elif suffix == \".docx\" and not docx_as_text:\n",
    "                log_step(\"Extracting slots from DOCX\", \"Analyzing document...\")\n",
    "                slots_payload = extract_slots_from_docx(input_path)\n",
    "                slots_tmp = tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False, suffix=\".json\")\n",
    "                json.dump(slots_payload, slots_tmp)\n",
    "                slots_tmp.flush()\n",
    "                log_step(\"Building answer generator\", \"Preparing answer generator...\")\n",
    "                gen = build_generator(\n",
    "                    search_mode,\n",
    "                    fund,\n",
    "                    int(k_max_hits),\n",
    "                    length_opt,\n",
    "                    int(approx_words) if approx_words else None,\n",
    "                    float(min_confidence),\n",
    "                    include_citations,\n",
    "                    llm,\n",
    "                    extra_docs,\n",
    "                )\n",
    "                log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                answers_dict = {}\n",
    "                slot_list = slots_payload.get('slots', [])\n",
    "                progress = st.progress(0)\n",
    "                qa_box = st.container() if show_live else None\n",
    "                for i, slot in enumerate(slot_list, 1):\n",
    "                    question = (slot.get(\"question_text\") or \"\").strip()\n",
    "                    if show_live and question:\n",
    "                        qa_box.markdown(f\"**Q{i}:** {question}\")\n",
    "                    ans = gen(question)\n",
    "                    answers_dict[slot.get('id', f'slot_{i}')] = ans\n",
    "                    progress.progress(i / len(slot_list), text=f\"{i}/{len(slot_list)}\")\n",
    "                    if show_live:\n",
    "                        text = ans.get('text', '') if isinstance(ans, dict) else ans\n",
    "                        qa_box.markdown(f\"**A{i}:** {text}\")\n",
    "                answers_tmp = tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False, suffix=\".json\")\n",
    "                json.dump({'by_id': answers_dict}, answers_tmp)\n",
    "                answers_tmp.flush()\n",
    "                out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".docx\")\n",
    "                apply_answers_to_docx(\n",
    "                    docx_path=input_path,\n",
    "                    slots_json_path=slots_tmp.name,\n",
    "                    answers_json_path=answers_tmp.name,\n",
    "                    out_path=out_tmp.name,\n",
    "                    mode=docx_write_mode,\n",
    "                    generator=None,\n",
    "                    gen_name=\"streamlit_app:rag_gen\",\n",
    "                )\n",
    "                with open(out_tmp.name, \"rb\") as f:\n",
    "                    st.download_button(\n",
    "                        \"Download answered DOCX\",\n",
    "                        f,\n",
    "                        file_name=Path(uploaded.name).stem + \"_answered.docx\",\n",
    "                    )\n",
    "            else:\n",
    "                log_step(\"Loading input text\", \"Reading document...\")\n",
    "                raw = load_input_text(input_path)\n",
    "                log_step(\"Extracting questions\", \"Finding questions...\")\n",
    "                questions = extract_questions(raw, llm)\n",
    "                log_step(\"Starting question-answering\", \"Generating answers...\")\n",
    "                answers = []\n",
    "                comments = []\n",
    "                total_qs = len(questions)\n",
    "                progress = st.progress(0)\n",
    "                qa_box = st.container() if show_live else None\n",
    "                for i, q in enumerate(questions, 1):\n",
    "                    if show_live and q:\n",
    "                        qa_box.markdown(f\"**Q{i}:** {q}\")\n",
    "                    ans, cmts = answer_question(\n",
    "                        q,\n",
    "                        search_mode,\n",
    "                        fund,\n",
    "                        int(k_max_hits),\n",
    "                        length_opt,\n",
    "                        int(approx_words) if approx_words else None,\n",
    "                        float(min_confidence),\n",
    "                        llm,\n",
    "                    )\n",
    "                    if not include_citations:\n",
    "                        ans = re.sub(r\"\\[\\d+\\]\", \"\", ans)\n",
    "                        cmts = []\n",
    "                    answers.append(ans)\n",
    "                    comments.append(cmts)\n",
    "                    progress.progress(i / total_qs, text=f\"{i}/{total_qs}\")\n",
    "                    if show_live:\n",
    "                        qa_box.markdown(f\"**A{i}:** {ans}\")\n",
    "                qa_doc = build_docx(\n",
    "                    questions,\n",
    "                    answers,\n",
    "                    comments,\n",
    "                    include_comments=include_citations,\n",
    "                )\n",
    "                out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".docx\")\n",
    "                out_tmp.write(qa_doc)\n",
    "                out_tmp.flush()\n",
    "                with open(out_tmp.name, \"rb\") as f:\n",
    "                    st.download_button(\n",
    "                        \"Download Q/A report\",\n",
    "                        f,\n",
    "                        file_name=Path(uploaded.name).stem + \"_answered.docx\",\n",
    "                    )\n",
    "            step_bar.progress(1.0, text=\"Done\")\n",
    "        elif run_clicked and not fund:\n",
    "            st.warning(\"Please select a fund or strategy before running.\")\n",
    "        elif run_clicked:\n",
    "            st.warning(\"Please upload a document before running.\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
